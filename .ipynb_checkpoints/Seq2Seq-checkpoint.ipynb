{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1167: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------Successfully processed HGNCDictIndex--------\n",
      "No. Entities: 42463\n",
      "Index Information: Counter({1: 170542, 2: 2194, 3: 241, 4: 48, 5: 13, 6: 12, 7: 5, 8: 3, 1167: 1, 10: 1, 9: 1, 11: 1})\n",
      "--------Successfully processed ChEBIDictIndex--------\n",
      "No. Entities: 53905\n",
      "Index Information: Counter({1: 221524, 2: 7348, 3: 508, 4: 97, 5: 21, 6: 8, 7: 5, 8: 1})\n",
      "--------Successfully processed GOBPDictIndex--------\n",
      "No. Entities: 30662\n",
      "Index Information: Counter({1: 122846, 2: 583, 3: 91, 4: 14, 5: 4, 6: 3})\n",
      "--------Successfully processed MESHDictIndex--------\n",
      "No. Entities: 4782\n",
      "Index Information: Counter({1: 53615})\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "import csv, re, copy, random\n",
    "import pickle\n",
    "import tensorflow as tf\n",
    "import tensorlayer as tl\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorlayer.layers import *\n",
    "from sklearn.utils import shuffle\n",
    "from time import gmtime, strftime\n",
    "import numpy as np\n",
    "import time\n",
    "import logging\n",
    "from collections import Counter\n",
    "from gensim.models import KeyedVectors\n",
    "from pycorenlp import StanfordCoreNLP\n",
    "from Dictionary import *\n",
    "from nltk.tree import Tree\n",
    "nlp = StanfordCoreNLP('http://localhost:9000')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadSentences(filename):\n",
    "    f = open(filename, encoding=\"utf8\")\n",
    "    reader = csv.DictReader(f, delimiter='\\t')\n",
    "    sentences = [row for row in reader]\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadAllTextSentences():\n",
    "    sentences = loadSentences('dataset/Training.sentence')\n",
    "    sentences.extend(loadSentences('dataset/SampleSet.sentence'))\n",
    "    sentences.extend(loadSentences('dataset/Task1NeuV3_corrected.sentence'))\n",
    "    # print(len(sentences))\n",
    "    TextSentenceID = dict()\n",
    "    vocabulary = set()\n",
    "    for line in sentences:\n",
    "        id = line['Sentence-ID'][4:]\n",
    "        text = line['Sentence']\n",
    "        output = nlp.annotate(text, properties={'annotators': 'tokenize', 'outputFormat': 'json'})\n",
    "        if id not in TextSentenceID:\n",
    "            TextSentenceID[id] = {'id': id,\n",
    "                                  'text': text,\n",
    "                                  'pmid': line['PMID'],\n",
    "                                  'tokens': [i['originalText'] for i in output['tokens']]}\n",
    "            vocabulary = vocabulary.union(set(TextSentenceID[id]['tokens']))\n",
    "    #     else:\n",
    "    #         assert (TextSentenceID[id]['text'] == line['Sentence']), 'ID: %s \\n Text1: %s \\n Text2: %s'%(id, TextSentenceID[id]['text'], line['Sentence']) \n",
    "    #         assert (TextSentenceID[id]['pmid'] == line['PMID']), 'ID: %s \\n PMID1: %s \\n PMID2: %s'%(id, TextSentenceID[id]['pmid'], line['PMID'])\n",
    "    print('Downloaded text sentences:', len(TextSentenceID), 'sentences')\n",
    "    print('Total vocabulary:', len(vocabulary), 'words')\n",
    "    return TextSentenceID, vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokeniseBEL(bs, mapToHGNC = False):\n",
    "    bs = re.subn(r',GOCCID:\\d+', '', bs) # Replace GOCCID (additional parameters of tloc)\n",
    "    bs = re.subn(r',sub\\([\\w,]*?\\)', '', bs[0]) # Remove all sub(_,_,_)\n",
    "    bs = re.subn(r',trunc\\(\\d+\\)', '', bs[0]) # Remove all trunc(_)\n",
    "    terms = re.findall(r'(a|bp|path|g|m|r|p)\\(([A-Z]+)\\:([^\"]*?|\".*?\")(,pmod\\(.*?\\))?\\)', bs[0])\n",
    "    # print(terms)\n",
    "    bs = re.subn(r'(a|bp|path|g|m|r|p)\\(([A-Z]+)\\:([^\"]*?|\".*?\")(,pmod\\(.*?\\))?\\)', '@', bs[0])\n",
    "    assert bs[1] == len(terms)\n",
    "    # print(bs)\n",
    "    relations = re.findall(r'\\s((?:->)|(?:-\\|)|(?:increases)|(?:decreases)|(?:directlyIncreases)|(?:directlyDecreases))\\s', bs[0])\n",
    "    # print(relations)\n",
    "    bs = re.subn(r'\\s((?:->)|(?:-\\|)|(?:increases)|(?:decreases)|(?:directlyIncreases)|(?:directlyDecreases))\\s', '&', bs[0])\n",
    "    assert bs[1] == len(relations)\n",
    "    # print(bs)\n",
    "    functions = re.findall(r'((?:act)|(?:complex)|(?:tloc)|(?:deg)|(?:kin)|(?:tscript)|(?:cat)|(?:sec)|(?:chap)|(?:gtp)|(?:pep)|(?:phos)|(?:ribo)|(?:tport)|(?:surf))', bs[0])\n",
    "    # print(functions)\n",
    "    bs = re.subn(r'((?:act)|(?:complex)|(?:tloc)|(?:deg)|(?:kin)|(?:tscript)|(?:cat)|(?:sec)|(?:chap)|(?:gtp)|(?:pep)|(?:phos)|(?:ribo)|(?:tport)|(?:surf))', '$', bs[0])\n",
    "    assert bs[1] == len(functions)\n",
    "    # print(bs[0]) # Term = @, Function = $, Relation = &\n",
    "    bs = re.subn(r', ', ',', bs[0]) # Remove white space after comma\n",
    "    template = bs[0]\n",
    "    return stringToTokens(template, terms, relations, functions, mapToHGNC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def stringToTokens(template, terms, relations, functions, mapToHGNC = False):\n",
    "    aList = []\n",
    "    typeDict = {'p':'p', 'bp':'bp', 'path':'path', 'a':'a', 'g':'p', 'm':'p', 'r':'p'}\n",
    "    relationDict = {'->': ' -> ', '-|': ' -| ', 'increases':' -> ', 'decreases':' -| ', 'directlyIncreases':' -> ', 'directlyDecreases':' -| '}\n",
    "    functionDict = {'act':'act', 'kin':'act', 'tscript':'act', 'cat':'act', 'chap':'act', 'gtp':'act', 'pep':'act', 'phos':'act', 'ribo':'act', 'tport':'act',\n",
    "                    'complex':'complex', \n",
    "                    'tloc': 'tloc', 'sec':'tloc', 'surf':'tloc', \n",
    "                    'deg': 'deg'}\n",
    "    for s in template:\n",
    "        if s == '@': # Term\n",
    "            termTuple = terms.pop(0)\n",
    "            ns = termTuple[1]\n",
    "            symbol = termTuple[2]\n",
    "            if mapToHGNC:\n",
    "                if ns == 'EGID': # Transform namespace (EGID to HGNC)\n",
    "                    if symbol in EGID2HGNC:\n",
    "                        ns = 'HGNC'\n",
    "                        symbol = EGID2HGNC[symbol]\n",
    "                        if any([a in symbol for a in [' ', '(', ')', '+', '-']]):\n",
    "                            symbol = '\"' + symbol + '\"'\n",
    "                    else:\n",
    "                        return False\n",
    "                elif ns == 'MGI': # Transform namespace (MGI to HGNC)\n",
    "                    if symbol in MGI2HGNC:\n",
    "                        ns = 'HGNC'\n",
    "                        symbol = MGI2HGNC[symbol]\n",
    "                        if any([a in symbol for a in [' ', '(', ')', '+', '-']]):\n",
    "                            symbol = '\"' + symbol + '\"'\n",
    "                    else:\n",
    "                        return False\n",
    "            if termTuple[3] == '':\n",
    "                aList.extend([typeDict[termTuple[0]], '(', ns+':'+symbol, ')'])\n",
    "            else:\n",
    "                aList.extend([typeDict[termTuple[0]], '(', ns+':'+symbol, ',', 'pmod(P)', ')'])\n",
    "        elif s == '&': # Relation\n",
    "            aList.append(relationDict[relations.pop(0)])\n",
    "        elif s == '$': # Function\n",
    "            aList.append(functionDict[functions.pop(0)])\n",
    "        else: # brackets and comma\n",
    "            aList.append(s)\n",
    "    return aList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def averageWordVectors(sentence, word_vectors):\n",
    "    output = nlp.annotate(sentence, properties={'annotators': 'tokenize', 'outputFormat': 'json'})\n",
    "    sentence_tokens = [i['originalText'] for i in output['tokens']]\n",
    "    sentence_matrix = np.array([word_vectors[tok] for tok in sentence_tokens if tok in word_vectors.vocab])\n",
    "#     print(len(sentence_matrix))\n",
    "    if len(sentence_matrix) > 0:\n",
    "        average_sentence_vector = np.mean(sentence_matrix, axis = 0)\n",
    "    else:\n",
    "        average_sentence_vector = np.random.uniform(-0.1, 0.1, emb_dim)\n",
    "    return average_sentence_vector    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def words2index(listOfWords, w2idx):\n",
    "    lst = []\n",
    "    for w in listOfWords:\n",
    "        if w in w2idx:\n",
    "            lst.append(w2idx[w])\n",
    "        else:\n",
    "            lst.append(w2idx['unk'])\n",
    "#             print(listOfWords)\n",
    "            if ' -> ' in w2idx:\n",
    "                print('Word in BEL converted to unknown:', w)\n",
    "            else:\n",
    "                print('Word in Text converted to unknown:', w)\n",
    "    return lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def validate():\n",
    "    numTest = 30\n",
    "    with g.as_default() as graph:\n",
    "        sumLoss = []\n",
    "        i = 1\n",
    "        random.shuffle(sampleZipSort)\n",
    "        validation_data = sampleZipSort[:numTest]\n",
    "        for enc_seq, tar_seq in validation_data:\n",
    "            x = enc_seq[::-1]\n",
    "            sentence, logits = predict(x, val_network_encode, val_network_decode, net_val, encode_seqs3, decode_seqs3)\n",
    "            logits = logits.reshape((logits.shape[0]*logits.shape[1], logits.shape[2]))\n",
    "            print(''.join(sentence), logits.shape)\n",
    "            tar_seq = tar_seq + [end_id]\n",
    "            \n",
    "            if len(logits) > len(tar_seq):\n",
    "                tar_seq = tar_seq + [end_id]*(len(logits) - len(tar_seq))\n",
    "            elif len(logits) < len(tar_seq):\n",
    "                end_logit = np.zeros((len(tar_seq)-len(logits), logits.shape[1]), dtype=np.float32)\n",
    "                end_logit[:, end_id:end_id+1] = 1\n",
    "                logits = np.concatenate((logits, end_logit), axis = 0) \n",
    "            \n",
    "#             print(len(tar_seq), logits.shape)\n",
    "            tar_mark = [1]*len(tar_seq)\n",
    "\n",
    "            val_loss = tl.cost.cross_entropy_seq_with_mask(logits=logits, target_seqs=target_seqs3, input_mask=target_mask3, return_details=False, name='cost_val')\n",
    "            lossX = sess.run([val_loss], {target_seqs3: [tar_seq], target_mask3: [tar_mark]})\n",
    "            print('Error', i, lossX)\n",
    "            sumLoss += lossX\n",
    "            i += 1\n",
    "        return sum(sumLoss)/len(sumLoss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(enc_seq, encode_layer, decode_layer, output_layer, encode_seqs, decode_seqs, needLogits = True, needAttention = False):\n",
    "    state, memory = sess.run([encode_layer.final_state, encode_layer.outputs], \n",
    "                             {encode_seqs: [enc_seq]})\n",
    "    feed_dict = {decode_layer.initial_state.cell_state:state,\n",
    "                 decode_layer.memory:memory,\n",
    "                 decode_seqs: [[start_id]]}\n",
    "    o, state = sess.run([tf.nn.softmax(output_layer.outputs), decode_layer.final_state], feed_dict)\n",
    "    \n",
    "    w_id = tl.nlp.sample_top(o[0], top_k=1)\n",
    "    w = B_idx2w[w_id]\n",
    "    sentence = [w]\n",
    "    logits = [o]\n",
    "    attention = state.alignments\n",
    "    for _ in range(30): # max sentence length\n",
    "        o, state = sess.run([tf.nn.softmax(output_layer.outputs), decode_layer.final_state],\n",
    "                        {decode_layer.initial_state:state,\n",
    "                         decode_layer.memory:memory,\n",
    "                         decode_seqs: [[w_id]]})\n",
    "        attention = np.concatenate((attention, state.alignments), axis = 0)\n",
    "        if needLogits:\n",
    "            logits.append(o)\n",
    "        w_id = tl.nlp.sample_top(o[0], top_k=1)\n",
    "        w = B_idx2w[w_id] \n",
    "        if w_id == end_id:\n",
    "#             print(attention)\n",
    "#             print(attention.shape)\n",
    "            break\n",
    "        sentence = sentence + [w]\n",
    "    if needLogits and needAttention:\n",
    "        return sentence, np.array(logits), attention\n",
    "    elif needLogits:\n",
    "        return sentence, np.array(logits)\n",
    "    elif needAttention:\n",
    "        return sentence, attention\n",
    "    else:\n",
    "        return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model(encode_seqs, decode_seqs, is_train=True, reuse=False):\n",
    "    with tf.variable_scope(\"model\", reuse=reuse):\n",
    "        # for translation, you need 2 seperated embedding layers\n",
    "        with tf.variable_scope(\"embedding\") as vs:\n",
    "            net_encode = EmbeddingInputlayer(\n",
    "                inputs = encode_seqs,\n",
    "                vocabulary_size = T_vocab_size_total,\n",
    "                embedding_size = emb_dim,\n",
    "                name = 'encode_seq_embedding')\n",
    "            net_decode = EmbeddingInputlayer(\n",
    "                inputs = decode_seqs,\n",
    "                vocabulary_size = B_vocab_size_total,\n",
    "                embedding_size = emb_dim,\n",
    "                name = 'decode_seq_embedding')\n",
    "            vs.reuse_variables()\n",
    "            tl.layers.set_name_reuse(True)\n",
    "        net_rnn = Seq2Seq(net_encode, net_decode,\n",
    "                cell_fn = tf.contrib.rnn.BasicLSTMCell,\n",
    "                n_hidden = emb_dim,\n",
    "                initializer = tf.random_uniform_initializer(-0.1, 0.1),\n",
    "                encode_sequence_length = retrieve_seq_length_op2(encode_seqs),\n",
    "                decode_sequence_length = retrieve_seq_length_op2(decode_seqs),\n",
    "                initial_state_encode = None,\n",
    "                dropout = (0.5 if is_train else None),\n",
    "                n_layer = 1,\n",
    "                return_seq_2d = True,\n",
    "                name = 'seq2seq')\n",
    "        net_out = DenseLayer(net_rnn, n_units=B_vocab_size_total, act=tf.identity, name='output')\n",
    "    return net_out, net_rnn, net_encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def modelWithAttention(encode_seqs, decode_seqs, is_train=True, reuse=False):\n",
    "    with tf.variable_scope(\"model\", reuse=reuse):\n",
    "        # for translation, you need 2 seperated embedding layers\n",
    "        with tf.variable_scope(\"embedding\") as vs:\n",
    "            net_encode = EmbeddingInputlayer(\n",
    "                inputs = encode_seqs,\n",
    "                vocabulary_size = T_vocab_size_total,\n",
    "                embedding_size = emb_dim,\n",
    "                name = 'encode_seq_embedding')\n",
    "            net_decode = EmbeddingInputlayer(\n",
    "                inputs = decode_seqs,\n",
    "                vocabulary_size = B_vocab_size_total,\n",
    "                embedding_size = emb_dim,\n",
    "                name = 'decode_seq_embedding')\n",
    "            vs.reuse_variables()\n",
    "            tl.layers.set_name_reuse(True)\n",
    "        network_encode = DynamicRNNLayer(\n",
    "            net_encode,\n",
    "            cell_fn=tf.contrib.rnn.BasicLSTMCell,\n",
    "            n_hidden=emb_dim,\n",
    "            initializer=tf.random_uniform_initializer(-0.1, 0.1),\n",
    "            initial_state=None,\n",
    "            dropout=(0.5 if is_train else None),\n",
    "            n_layer=1,\n",
    "            sequence_length=retrieve_seq_length_op2(encode_seqs),\n",
    "            return_last=False,\n",
    "            return_seq_2d=False,\n",
    "            name='seq2seq_encode')\n",
    "        network_decode = DynamicRNNLayerWithAttention(\n",
    "            net_decode,\n",
    "            cell_fn=tf.contrib.rnn.BasicLSTMCell,\n",
    "            attention_mechanism_fn=tf.contrib.seq2seq.LuongAttention,\n",
    "            memory=network_encode.outputs,\n",
    "            n_hidden=emb_dim,\n",
    "            initializer=tf.random_uniform_initializer(-0.1, 0.1),\n",
    "            initial_state=network_encode.final_state,\n",
    "           # initial_state=(network_encode.final_state if is_train else None),\n",
    "            dropout=(0.5 if is_train else None),\n",
    "            n_layer=1,\n",
    "            sequence_length=retrieve_seq_length_op2(decode_seqs),\n",
    "            return_last=False,\n",
    "            return_seq_2d=True,\n",
    "            name='seq2seq_decode')\n",
    "        net_out = DenseLayer(network_decode, n_units=B_vocab_size_total, act=tf.identity, name='output')\n",
    "    return net_out, network_encode, net_encode, network_decode, network_decode.cell, net_decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DynamicRNNLayerWithAttention(Layer):\n",
    "    \"\"\"\n",
    "    The :class:`DynamicRNNLayer` class is a dynamic recurrent layer, see ``tf.nn.dynamic_rnn``.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    layer : :class:`Layer`\n",
    "        Previous layer\n",
    "    cell_fn : TensorFlow cell function\n",
    "        A TensorFlow core RNN cell\n",
    "            - See `RNN Cells in TensorFlow <https://www.tensorflow.org/api_docs/python/>`__\n",
    "            - Note TF1.0+ and TF1.0- are different\n",
    "    cell_init_args : dictionary or None\n",
    "        The arguments for the cell function.\n",
    "    n_hidden : int\n",
    "        The number of hidden units in the layer.\n",
    "    initializer : initializer\n",
    "        The initializer for initializing the parameters.\n",
    "    sequence_length : tensor, array or None\n",
    "        The sequence length of each row of input data, see ``Advanced Ops for Dynamic RNN``.\n",
    "            - If None, it uses ``retrieve_seq_length_op`` to compute the sequence length, i.e. when the features of padding (on right hand side) are all zeros.\n",
    "            - If using word embedding, you may need to compute the sequence length from the ID array (the integer features before word embedding) by using ``retrieve_seq_length_op2`` or ``retrieve_seq_length_op``.\n",
    "            - You can also input an numpy array.\n",
    "            - More details about TensorFlow dynamic RNN in `Wild-ML Blog <http://www.wildml.com/2016/08/rnns-in-tensorflow-a-practical-guide-and-undocumented-features/>`__.\n",
    "    initial_state : None or RNN State\n",
    "        If None, `initial_state` is zero state.\n",
    "    dropout : tuple of float or int\n",
    "        The input and output keep probability (input_keep_prob, output_keep_prob).\n",
    "            - If one int, input and output keep probability are the same.\n",
    "    n_layer : int\n",
    "        The number of RNN layers, default is 1.\n",
    "    return_last : boolean or None\n",
    "        Whether return last output or all outputs in each step.\n",
    "            - If True, return the last output, \"Sequence input and single output\"\n",
    "            - If False, return all outputs, \"Synced sequence input and output\"\n",
    "            - In other word, if you want to stack more RNNs on this layer, set to False.\n",
    "    return_seq_2d : boolean\n",
    "        Only consider this argument when `return_last` is `False`\n",
    "            - If True, return 2D Tensor [n_example, n_hidden], for stacking DenseLayer after it.\n",
    "            - If False, return 3D Tensor [n_example/n_steps, n_steps, n_hidden], for stacking multiple RNN after it.\n",
    "    dynamic_rnn_init_args : dictionary\n",
    "        The arguments for ``tf.nn.dynamic_rnn``.\n",
    "    name : str\n",
    "        A unique layer name.\n",
    "\n",
    "    Attributes\n",
    "    ------------\n",
    "    outputs : tensor\n",
    "        The output of this layer.\n",
    "\n",
    "    final_state : tensor or StateTuple\n",
    "        The finial state of this layer.\n",
    "            - When `state_is_tuple` is `False`, it is the final hidden and cell states, `states.get_shape() = [?, 2 * n_hidden]`.\n",
    "            - When `state_is_tuple` is `True`, it stores two elements: `(c, h)`.\n",
    "            - In practice, you can get the final state after each iteration during training, then feed it to the initial state of next iteration.\n",
    "\n",
    "    initial_state : tensor or StateTuple\n",
    "        The initial state of this layer.\n",
    "            - In practice, you can set your state at the begining of each epoch or iteration according to your training procedure.\n",
    "\n",
    "    batch_size : int or tensor\n",
    "        It is an integer, if it is able to compute the `batch_size`; otherwise, tensor for dynamic batch size.\n",
    "\n",
    "    sequence_length : a tensor or array\n",
    "        The sequence lengths computed by Advanced Opt or the given sequence lengths, [batch_size]\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    Input dimension should be rank 3 : [batch_size, n_steps(max), n_features], if no, please see :class:`ReshapeLayer`.\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    Synced sequence input and output, for loss function see ``tl.cost.cross_entropy_seq_with_mask``.\n",
    "\n",
    "    >>> input_seqs = tf.placeholder(dtype=tf.int64, shape=[batch_size, None], name=\"input\")\n",
    "    >>> net = tl.layers.EmbeddingInputlayer(\n",
    "    ...             inputs = input_seqs,\n",
    "    ...             vocabulary_size = vocab_size,\n",
    "    ...             embedding_size = embedding_size,\n",
    "    ...             name = 'seq_embedding')\n",
    "    >>> net = tl.layers.DynamicRNNLayer(net,\n",
    "    ...             cell_fn = tf.contrib.rnn.BasicLSTMCell, # for TF0.2 use tf.nn.rnn_cell.BasicLSTMCell,\n",
    "    ...             n_hidden = embedding_size,\n",
    "    ...             dropout = (0.7 if is_train else None),\n",
    "    ...             sequence_length = tl.layers.retrieve_seq_length_op2(input_seqs),\n",
    "    ...             return_seq_2d = True,                   # stack denselayer or compute cost after it\n",
    "    ...             name = 'dynamicrnn')\n",
    "    ... net = tl.layers.DenseLayer(net, n_units=vocab_size, name=\"output\")\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    - `Wild-ML Blog <http://www.wildml.com/2016/08/rnns-in-tensorflow-a-practical-guide-and-undocumented-features/>`__\n",
    "    - `dynamic_rnn.ipynb <https://github.com/dennybritz/tf-rnn/blob/master/dynamic_rnn.ipynb>`__\n",
    "    - `tf.nn.dynamic_rnn <https://github.com/tensorflow/tensorflow/blob/master/tensorflow/g3doc/api_docs/python/functions_and_classes/shard8/tf.nn.dynamic_rnn.md>`__\n",
    "    - `tflearn rnn <https://github.com/tflearn/tflearn/blob/master/tflearn/layers/recurrent.py>`__\n",
    "    - ``tutorial_dynamic_rnn.py``\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            layer,\n",
    "            cell_fn,  #tf.nn.rnn_cell.LSTMCell,\n",
    "            attention_mechanism_fn,\n",
    "            memory,\n",
    "            cell_init_args=None,\n",
    "            n_hidden=256,\n",
    "            initializer=tf.random_uniform_initializer(-0.1, 0.1),\n",
    "            sequence_length=None,\n",
    "            initial_state=None,\n",
    "            dropout=None,\n",
    "            n_layer=1,\n",
    "            return_last=None,\n",
    "            return_seq_2d=False,\n",
    "            dynamic_rnn_init_args=None,\n",
    "            name='dyrnn',\n",
    "    ):\n",
    "#         self.initial_state_from_encoder = initial_state\n",
    "        \n",
    "        if dynamic_rnn_init_args is None:\n",
    "            dynamic_rnn_init_args = {}\n",
    "        if cell_init_args is None:\n",
    "            cell_init_args = {'state_is_tuple': True}\n",
    "        if return_last is None:\n",
    "            return_last = True\n",
    "\n",
    "        Layer.__init__(self, name=name)\n",
    "        if cell_fn is None:\n",
    "            raise Exception(\"Please put in cell_fn\")\n",
    "        if 'GRU' in cell_fn.__name__:\n",
    "            try:\n",
    "                cell_init_args.pop('state_is_tuple')\n",
    "            except Exception:\n",
    "                logging.warning(\"pop state_is_tuple fails.\")\n",
    "        self.inputs = layer.outputs\n",
    "        self.memory = memory\n",
    "        print(\"  [TL] DynamicRNNLayerWithAttention %s: n_hidden:%d, in_dim:%d in_shape:%s cell_fn:%s dropout:%s n_layer:%d\" %\n",
    "                     (self.name, n_hidden, self.inputs.get_shape().ndims, self.inputs.get_shape(), cell_fn.__name__, dropout, n_layer))\n",
    "\n",
    "        logging.info(\"DynamicRNNLayerWithAttention %s: n_hidden:%d, in_dim:%d in_shape:%s cell_fn:%s dropout:%s n_layer:%d\" %\n",
    "                     (self.name, n_hidden, self.inputs.get_shape().ndims, self.inputs.get_shape(), cell_fn.__name__, dropout, n_layer))\n",
    "\n",
    "        # Input dimension should be rank 3 [batch_size, n_steps(max), n_features]\n",
    "        try:\n",
    "            self.inputs.get_shape().with_rank(3)\n",
    "        except Exception:\n",
    "            raise Exception(\"RNN : Input dimension should be rank 3 : [batch_size, n_steps(max), n_features]\")\n",
    "\n",
    "        # Get the batch_size\n",
    "        fixed_batch_size = self.inputs.get_shape().with_rank_at_least(1)[0]\n",
    "        if fixed_batch_size.value:\n",
    "            batch_size = fixed_batch_size.value\n",
    "            logging.info(\"       batch_size (concurrent processes): %d\" % batch_size)\n",
    "        else:\n",
    "            from tensorflow.python.ops import array_ops\n",
    "            batch_size = array_ops.shape(self.inputs)[0]\n",
    "            logging.info(\"       non specified batch_size, uses a tensor instead.\")\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "        # Creats the cell function\n",
    "        # cell_instance_fn=lambda: cell_fn(num_units=n_hidden, **cell_init_args) # HanSheng\n",
    "        rnn_creator = lambda: cell_fn(num_units=n_hidden, **cell_init_args)\n",
    "        \n",
    "        # ============================ PLJ added attention mechanism ====================================\n",
    "        attention_mechanism = attention_mechanism_fn(num_units = n_hidden,\n",
    "                                memory = self.memory,\n",
    "                                memory_sequence_length = None # Might be made more accurate later \n",
    "                                )\n",
    "        attn_cell_creator = lambda: tf.contrib.seq2seq.AttentionWrapper(rnn_creator(), attention_mechanism)\n",
    "        # ===============================================================================================            \n",
    "        # Apply dropout\n",
    "        if dropout:\n",
    "            if isinstance(dropout, (tuple, list)):\n",
    "                in_keep_prob = dropout[0]\n",
    "                out_keep_prob = dropout[1]\n",
    "            elif isinstance(dropout, float):\n",
    "                in_keep_prob, out_keep_prob = dropout, dropout\n",
    "            else:\n",
    "                raise Exception(\"Invalid dropout type (must be a 2-D tuple of \" \"float)\")\n",
    "            try:  # TF1.0\n",
    "                DropoutWrapper_fn = tf.contrib.rnn.DropoutWrapper\n",
    "            except Exception:\n",
    "                DropoutWrapper_fn = tf.nn.rnn_cell.DropoutWrapper\n",
    "\n",
    "            # cell_instance_fn1=cell_instance_fn        # HanSheng\n",
    "            # cell_instance_fn=DropoutWrapper_fn(\n",
    "            #                     cell_instance_fn1(),\n",
    "            #                     input_keep_prob=in_keep_prob,\n",
    "            #                     output_keep_prob=out_keep_prob)\n",
    "            cell_creator = lambda is_last=True: \\\n",
    "                    DropoutWrapper_fn(attn_cell_creator(),\n",
    "                                      input_keep_prob=in_keep_prob,\n",
    "                                      output_keep_prob=out_keep_prob if is_last else 1.0)\n",
    "        else:\n",
    "            cell_creator = attn_cell_creator\n",
    "        self.cell = cell_creator()\n",
    "        # Apply multiple layers\n",
    "        if n_layer > 1:\n",
    "            try:\n",
    "                MultiRNNCell_fn = tf.contrib.rnn.MultiRNNCell\n",
    "            except Exception:\n",
    "                MultiRNNCell_fn = tf.nn.rnn_cell.MultiRNNCell\n",
    "\n",
    "            # cell_instance_fn2=cell_instance_fn # HanSheng\n",
    "            try:\n",
    "                # cell_instance_fn=lambda: MultiRNNCell_fn([cell_instance_fn2() for _ in range(n_layer)], state_is_tuple=True) # HanSheng\n",
    "                self.cell = MultiRNNCell_fn([cell_creator(is_last=i == n_layer - 1) for i in range(n_layer)], state_is_tuple=True)\n",
    "            except Exception:  # when GRU\n",
    "                # cell_instance_fn=lambda: MultiRNNCell_fn([cell_instance_fn2() for _ in range(n_layer)]) # HanSheng\n",
    "                self.cell = MultiRNNCell_fn([cell_creator(is_last=i == n_layer - 1) for i in range(n_layer)])\n",
    "\n",
    "        # self.cell=cell_instance_fn() # HanSheng\n",
    "\n",
    "        # Initialize initial_state\n",
    "        if initial_state is None:\n",
    "            self.initial_state = self.cell.zero_state(batch_size, dtype=D_TYPE)  # dtype=tf.float32)\n",
    "        else:\n",
    "            try:\n",
    "                self.initial_state = self.cell.zero_state(batch_size, dtype=D_TYPE).clone(cell_state=initial_state) \n",
    "            except AttributeError:\n",
    "                self.initial_state = initial_state\n",
    "\n",
    "        # Computes sequence_length\n",
    "        if sequence_length is None:\n",
    "            try:  # TF1.0\n",
    "                sequence_length = retrieve_seq_length_op(self.inputs if isinstance(self.inputs, tf.Tensor) else tf.stack(self.inputs))\n",
    "            except Exception:  # TF0.12\n",
    "                sequence_length = retrieve_seq_length_op(self.inputs if isinstance(self.inputs, tf.Tensor) else tf.pack(self.inputs))\n",
    "        \n",
    "        # Main - Computes outputs and last_states\n",
    "#         with tf.variable_scope(tf.get_variable_scope()) as scope:\n",
    "        with tf.variable_scope(name, initializer=initializer) as vs:\n",
    "            outputs, last_states = tf.nn.dynamic_rnn(\n",
    "                cell=self.cell,\n",
    "                # inputs=X\n",
    "                inputs=self.inputs,\n",
    "                dtype=tf.float32,\n",
    "                sequence_length=sequence_length,\n",
    "                initial_state=self.initial_state,\n",
    "                **dynamic_rnn_init_args)\n",
    "            rnn_variables = tf.get_collection(TF_GRAPHKEYS_VARIABLES, scope=vs.name)\n",
    "\n",
    "            # logging.info(\"     n_params : %d\" % (len(rnn_variables)))\n",
    "            # Manage the outputs\n",
    "            if return_last:\n",
    "                # [batch_size, n_hidden]\n",
    "                # outputs = tf.transpose(tf.pack(outputs), [1, 0, 2]) # TF1.0 tf.pack --> tf.stack\n",
    "                self.outputs = advanced_indexing_op(outputs, sequence_length)\n",
    "            else:\n",
    "                # [batch_size, n_step(max), n_hidden]\n",
    "                # self.outputs = result[0][\"outputs\"]\n",
    "                # self.outputs = outputs    # it is 3d, but it is a list\n",
    "                if return_seq_2d:\n",
    "                    # PTB tutorial:\n",
    "                    # 2D Tensor [n_example, n_hidden]\n",
    "                    try:  # TF1.0\n",
    "                        self.outputs = tf.reshape(tf.concat(outputs, 1), [-1, n_hidden])\n",
    "                    except Exception:  # TF0.12\n",
    "                        self.outputs = tf.reshape(tf.concat(1, outputs), [-1, n_hidden])\n",
    "                else:\n",
    "                    # <akara>:\n",
    "                    # 3D Tensor [batch_size, n_steps(max), n_hidden]\n",
    "                    max_length = tf.shape(outputs)[1]\n",
    "                    batch_size = tf.shape(outputs)[0]\n",
    "\n",
    "                    try:  # TF1.0\n",
    "                        self.outputs = tf.reshape(tf.concat(outputs, 1), [batch_size, max_length, n_hidden])\n",
    "                    except Exception:  # TF0.12\n",
    "                        self.outputs = tf.reshape(tf.concat(1, outputs), [batch_size, max_length, n_hidden])\n",
    "                    # self.outputs = tf.reshape(tf.concat(1, outputs), [-1, max_length, n_hidden])\n",
    "#             tf.get_variable_scope().reuse_variables()\n",
    "\n",
    "        # Final state\n",
    "        self.final_state = last_states\n",
    "        \n",
    "        self.sequence_length = sequence_length\n",
    "\n",
    "        self.all_layers = list(layer.all_layers)\n",
    "        self.all_params = list(layer.all_params)\n",
    "        self.all_drop = dict(layer.all_drop)\n",
    "\n",
    "        self.all_layers.extend([self.outputs])\n",
    "        self.all_params.extend(rnn_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def createTextDict():\n",
    "    T_idx2w = ['_', 'unk'] + list(vocabulary) \n",
    "    for word in word_vectors.vocab.keys():\n",
    "        if word not in T_idx2w:\n",
    "            T_idx2w.append(word)\n",
    "        if len(T_idx2w) >= T_vocab_size + 2:\n",
    "            break\n",
    "    T_idx2w.extend(['start_id', 'end_id'])\n",
    "    T_w2idx = dict([(T_idx2w[i], i) for i in range(len(T_idx2w))])\n",
    "    T_vocab_size_total = len(T_idx2w)\n",
    "    print('Finish creating text dict (Total text vocab size = %d)'%T_vocab_size_total)\n",
    "    return T_idx2w, T_w2idx, T_vocab_size_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def createBELTokenDict():\n",
    "    B_idx2w = []\n",
    "    B_idx2w.extend(generalTokens)\n",
    "    B_idx2w.extend(['HGNC:'+x if all([a not in x for a in [' ', '(', ')', '+', '-']]) else 'HGNC:\"'+x+'\"' for x in HGNCDict.keys()])\n",
    "    B_idx2w.extend(['CHEBI:'+x if all([a not in x for a in [' ', '(', ')', '+', '-']]) else 'CHEBI:\"'+x+'\"' for x in ChEBIDict.keys()])\n",
    "    B_idx2w.extend(['GOBP:'+x if all([a not in x for a in [' ', '(', ')', '+', '-']]) else 'GOBP:\"'+x+'\"' for x in GOBPDict.keys()])\n",
    "    B_idx2w.extend(['MESHD:'+x if all([a not in x for a in [' ', '(', ')', '+', '-']]) else 'MESHD:\"'+x+'\"' for x in MESHDict.keys()])\n",
    "    B_idx2w.extend(['start_id', 'end_id'])\n",
    "    B_w2idx = dict([(B_idx2w[i], i) for i in range(len(B_idx2w))])\n",
    "    B_vocab_size_total = len(B_idx2w)\n",
    "    print('Finish creating BEL token dict (Total text vocab size = %d)'%B_vocab_size_total)\n",
    "    return B_idx2w, B_w2idx, B_vocab_size_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadWordEmbeddings():\n",
    "    if os.path.exists(\"word_embedding.pickle\"):\n",
    "        print('Downloading word embeddings from a file')\n",
    "        word_embedding = pickle.load(open(\"word_embedding.pickle\", \"rb\"))\n",
    "    else:\n",
    "        print('Cannot find the word embeddings file, so creating the embeddings from scratch.')\n",
    "        word_embedding = np.random.uniform(-0.1, 0.1, (2, emb_dim))\n",
    "        count = 2\n",
    "        for i in range(2, T_vocab_size_total-2):\n",
    "            if i%100 == 0:\n",
    "                print(i)\n",
    "            if T_idx2w[i] in word_vectors.vocab:\n",
    "        #         print(word_embedding.shape, np.array([word_vectors[T_idx2w[i]]]).shape)\n",
    "                word_embedding = np.append(word_embedding, [word_vectors[T_idx2w[i]]], axis = 0)\n",
    "            else:\n",
    "                word_embedding = np.append(word_embedding, [np.random.uniform(-0.1, 0.1, emb_dim)], axis = 0)\n",
    "                count += 1\n",
    "        word_embedding = np.append(word_embedding, [np.random.uniform(-0.1, 0.1, emb_dim)], axis = 0)\n",
    "        word_embedding = np.append(word_embedding, [np.random.uniform(-0.1, 0.1, emb_dim)], axis = 0)\n",
    "        print(count+2)\n",
    "        pickle.dump(word_embedding, open(\"word_embedding.pickle\", \"wb\"))\n",
    "    print('Finish loading word embeddings (shape: %s)' % (str(word_embedding.shape)))\n",
    "    return word_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def loadBELTokenEmbeddings():\n",
    "    if os.path.exists(\"BELToken_embedding.pickle\"):\n",
    "        print('Downloading BELToken embeddings from a file')\n",
    "        BELToken_embedding = pickle.load(open(\"BELToken_embedding.pickle\", \"rb\"))\n",
    "    else:\n",
    "        print('Cannot find the BELToken embeddings file, so creating the embeddings from scratch.')\n",
    "        BELToken_embedding = np.random.uniform(-0.1, 0.1, (len(generalTokens), emb_dim))\n",
    "        print(BELToken_embedding.shape)\n",
    "        for x in HGNCDict.keys():\n",
    "            if x in word_vectors.vocab:\n",
    "                BELToken_embedding = np.append(BELToken_embedding, [word_vectors[x]], axis = 0)\n",
    "            else:\n",
    "                BELToken_embedding = np.append(BELToken_embedding, [np.random.uniform(-0.1, 0.1, emb_dim)], axis = 0)\n",
    "        print(BELToken_embedding.shape)\n",
    "        for d in [ChEBIDict, GOBPDict, MESHDict]:\n",
    "            for x in d.keys():\n",
    "                BELToken_embedding = np.append(BELToken_embedding, [averageWordVectors(d[x]['definition'], word_vectors)], axis = 0)\n",
    "            print(BELToken_embedding.shape)\n",
    "        BELToken_embedding = np.append(BELToken_embedding, [np.random.uniform(-0.1, 0.1, emb_dim)], axis = 0)\n",
    "        BELToken_embedding = np.append(BELToken_embedding, [np.random.uniform(-0.1, 0.1, emb_dim)], axis = 0)\n",
    "        print(BELToken_embedding.shape)\n",
    "        assert BELToken_embedding.shape[0] == len(B_idx2w)\n",
    "        pickle.dump(BELToken_embedding, open(\"BELToken_embedding.pickle\", \"wb\"))\n",
    "    print('Finish loading BELToken embeddings (shape: %s)' % (str(BELToken_embedding.shape)))\n",
    "    return BELToken_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadTrainingData(filename):    \n",
    "    sentences = loadSentences(filename)\n",
    "    trainBSentences = []\n",
    "    trainBTokenized = []\n",
    "    trainTSentences = []\n",
    "    trainTTokenized = []\n",
    "    for line in sentences:\n",
    "        BELTokens = tokeniseBEL(line['BEL-normalised'], mapToHGNC = True)\n",
    "        if not BELTokens:\n",
    "            print('Deleted BEL sentence:', line['BEL-normalised'])\n",
    "            continue\n",
    "        trainBSentences.append(line['BEL-normalised'])\n",
    "        trainBTokenized.append(BELTokens)\n",
    "        trainTSentences.append(TextSentenceID[line['Sentence-ID'][4:]]['text'])\n",
    "        trainTTokenized.append(TextSentenceID[line['Sentence-ID'][4:]]['tokens'])\n",
    "    trainT1 = [words2index(sublist, T_w2idx) for sublist in trainTTokenized]\n",
    "    trainB1 = [words2index(sublist, B_w2idx) for sublist in trainBTokenized]\n",
    "    assert len(trainT1) == len(trainB1)\n",
    "    zipsort = sorted(zip(trainT1,trainB1), key=lambda pair: len(pair[0]))\n",
    "    trainT = [x for x, y in zipsort]\n",
    "    trainB = [y for x, y in zipsort] # Sort to make a batch have sentences with similar lengths\n",
    "#     print(trainB[150], trainT[150])\n",
    "    return trainT, trainB, zipsort"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def testModel(filename):\n",
    "    sentences = loadSentences(filename) # filename = 'dataset/Task1NeuV3_corrected.sentence' or 'dataset/SampleSet.sentence'\n",
    "    realFilename = filename.split('/')[1].split('.')[0]\n",
    "    sampleTSentences = [TextSentenceID[line['Sentence-ID'][4:]]['text'] for line in sentences]\n",
    "    sampleTTokenized = [TextSentenceID[line['Sentence-ID'][4:]]['tokens'] for line in sentences]\n",
    "    sampleT = [words2index(sublist, T_w2idx) for sublist in sampleTTokenized]\n",
    "    f = open('results/' + realFilename + '_' + strftime(\"%Y%m%d%H%M%S\", gmtime()) +'.txt', 'w')\n",
    "    for i in range(len(sampleT)):\n",
    "        sentence_id = sentences[i]['Sentence-ID'][4:]\n",
    "        seed_id = sampleT[i]\n",
    "        seed_id = seed_id[::-1]\n",
    "        sentence = predict(seed_id, net_rnn, network_decode, net, encode_seqs2, decode_seqs2, needLogits = False)\n",
    "        print(i, ''.join(sentence))\n",
    "        f.write(sentence_id + '\\t' + ''.join(sentence) + '\\n')\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def testInference(visualiseAttention = False):\n",
    "    seeds = [\"MMP-2 gelatinolytic activity was higher in cells infected with PBS (mock) and Ad-SV.\", \n",
    "             # p(HGNC:MMP2) increases act(p(HGNC:MMP2))\n",
    "            \"In contrast, vandetanib treatment induced a 2.3-fold increase in eNOS mRNA in B16.F10 vasculature.\", \n",
    "             # kin(p(MGI:Kdr)) decreases p(MGI:Nos3)\n",
    "            \"Thus, extramitochondrially targeted AIF is a dominant cell death inducer.\", \n",
    "             # p(MGI:Aifm1) increases bp(GOBP:\"cell death\")\n",
    "            \"Binding of PIAS1 to human AR DNA+ligand binding domains was androgen dependent in the yeast liquid beta-galactosidase assay.\", \n",
    "             # a(CHEBI:androgen) -> complex(p(HGNC:AR),p(HGNC:PIAS1))\"\n",
    "            \"The data suggest that genistein may inhibit CFTR by two mechanisms.\", \n",
    "             # a(CHEBI:genistein) decreases p(MGI:Cftr)\n",
    "            \"LPS-induced NO synthesis feedback regulates itself through up-regulation of OPN promoter activity and gene transcription.\" \n",
    "             # a(CHEBI:lipopolysaccharide) increases a(CHEBI:\"nitric oxide\"), p(MGI:Spp1) decreases a(CHEBI:\"nitric oxide\") \n",
    "            ] \n",
    "    for seed in seeds:\n",
    "        print(\"Input >\", seed)\n",
    "        output = nlp.annotate(seed, properties={'annotators': 'tokenize', 'outputFormat': 'json'})\n",
    "        seed_tokens = [i['originalText'] for i in output['tokens']]\n",
    "        print(seed_tokens)\n",
    "        seed_id = [T_w2idx[w] if w in T_w2idx else T_w2idx['unk'] for w in seed_tokens]\n",
    "        seed_id = seed_id[::-1]\n",
    "        if visualiseAttention:\n",
    "            sentence, attention = predict(seed_id, net_rnn, network_decode, net, encode_seqs2, decode_seqs2, needLogits = False, needAttention = True)\n",
    "        else:\n",
    "            sentence = predict(seed_id, net_rnn, network_decode, net, encode_seqs2, decode_seqs2, needLogits = False, needAttention = False)\n",
    "        print(''.join(sentence))\n",
    "        if visualiseAttention:\n",
    "            plt.imshow(np.flip(attention, axis = 1), cmap='gray') # Flip attention as we give a reverse sentence as an input.\n",
    "            plt.yticks(range(len(sentence)), sentence)\n",
    "            plt.xticks(range(len(seed_tokens)), seed_tokens, rotation='vertical')\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Global parameters\n",
    "emb_dim = 200\n",
    "batch_size = 16\n",
    "n_epoch = 50\n",
    "T_vocab_size = 50000\n",
    "generalTokens = ['_', 'unk', '(', ')', ',', 'p', 'a', 'bp', 'path', 'act', 'pmod(P)', 'tloc', 'complex', 'deg', ' -> ', ' -| ', ' -- ', 'PH:placeholder']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish creating BEL token dict (Total text vocab size = 131832)\n",
      "Downloading word embeddings from a file\n",
      "Finish loading word embeddings (shape: (50004, 200))\n",
      "Downloading BELToken embeddings from a file\n",
      "Finish loading BELToken embeddings (shape: (131832, 200))\n",
      "Deleted BEL sentence: a(CHEBI:corticotropin) -> p(MGI:Sik1,pmod(P))\n",
      "Deleted BEL sentence: act(p(EGID:12534)) -> p(HGNC:BTG1,pmod(P))\n",
      "Deleted BEL sentence: act(p(EGID:279561)) -> p(HGNC:SLC12A4,pmod(P))\n",
      "Deleted BEL sentence: act(p(EGID:279561)) -> p(HGNC:SLC12A4,pmod(P))\n",
      "Deleted BEL sentence: act(p(MGI:Adrbk2)) -> p(HGNC:OPRK1,pmod(P))\n",
      "Deleted BEL sentence: act(p(HGNC:SRPK1)) -> p(EGID:110809,pmod(P))\n",
      "Deleted BEL sentence: act(p(HGNC:SRPK2)) -> p(EGID:110809,pmod(P))\n",
      "Deleted BEL sentence: act(p(HGNC:TSSK6)) -> p(MGI:H3f3a,pmod(P))\n",
      "Deleted BEL sentence: p(EGID:103968,pmod(P)) -> act(p(HGNC:PNPLA2))\n",
      "Deleted BEL sentence: act(p(MGI:Ppp1cc)) -| p(HGNC:EIF2S1,pmod(P))\n",
      "Deleted BEL sentence: p(MGI:Cdc42) -| p(HGNC:AXIN1,pmod(P))\n",
      "Deleted BEL sentence: p(MGI:Cdc42) -> p(HGNC:GSK3B,pmod(P))\n",
      "Deleted BEL sentence: p(MGI:Il3) -> p(HGNC:BCL2,pmod(P))\n",
      "Deleted BEL sentence: p(MGI:Il3) -> p(HGNC:CRKL,pmod(P))\n",
      "Deleted BEL sentence: p(MGI:Irs3) -| p(HGNC:IRS1,pmod(P))\n",
      "Deleted BEL sentence: p(MGI:Irs3) -| p(HGNC:IRS2,pmod(P))\n",
      "Deleted BEL sentence: p(MGI:Irs4) -| p(HGNC:IRS1,pmod(P))\n",
      "Deleted BEL sentence: p(MGI:Irs4) -| p(HGNC:IRS2,pmod(P))\n",
      "Deleted BEL sentence: p(HGNC:RELA,pmod(P)) -| p(MGI:\"Rela\",pmod(P))\n",
      "Deleted BEL sentence: p(MGI:Sik1,pmod(P)) -| tloc(p(MGI:Sik1))\n",
      "Deleted BEL sentence: a(CHEBI:\"carbon monoxide\") -> p(MGI:Hspa1b)\n",
      "Deleted BEL sentence: a(CHEBI:indometacin) -| act(p(MGI:Cyp2d9))\n",
      "Deleted BEL sentence: a(CHEBI:indometacin) -| act(p(MGI:Cyp3a11))\n",
      "Deleted BEL sentence: a(CHEBI:\"lipoteichoic acid\") -> p(MGI:Adrbk1)\n",
      "Deleted BEL sentence: a(CHEBI:trifluoperazine) -| act(p(MGI:Calm1))\n",
      "Deleted BEL sentence: bp(GOBP:\"response to endoplasmic reticulum stress\") -> complex(p(MGI:Atf1),p(HGNC:CREB1))\n",
      "Deleted BEL sentence: bp(GOBP:\"response to osmotic stress\") -> p(MGI:Hnrnpa1)\n",
      "Deleted BEL sentence: bp(GOBP:\"wound healing\") -> p(MGI:Ifna2)\n",
      "Deleted BEL sentence: act(p(EGID:104263)) -| act(p(HGNC:ETV2))\n",
      "Deleted BEL sentence: act(p(EGID:116816)) -> act(p(HGNC:MAPK1))\n",
      "Deleted BEL sentence: act(p(EGID:116816)) -> act(p(HGNC:MAPK3))\n",
      "Deleted BEL sentence: act(p(EGID:116817)) -> act(p(HGNC:MAPK1))\n",
      "Deleted BEL sentence: act(p(EGID:116817)) -> act(p(HGNC:MAPK3))\n",
      "Deleted BEL sentence: act(p(EGID:330914)) -| act(p(MGI:Cdc42))\n",
      "Deleted BEL sentence: act(p(EGID:330914)) -| act(p(MGI:Cdc42))\n",
      "Deleted BEL sentence: act(p(EGID:330914)) -| act(p(HGNC:RAC1))\n",
      "Deleted BEL sentence: act(p(HGNC:CAPN2)) -> act(p(MGI:Casp12))\n",
      "Deleted BEL sentence: act(p(MGI:Cbs)) -> act(p(HGNC:ACAA1))\n",
      "Deleted BEL sentence: act(p(HGNC:CCR1)) -| p(MGI:Ccl6)\n",
      "Deleted BEL sentence: act(p(HGNC:CD44)) -| p(MGI:\"mt-Co1\")\n",
      "Deleted BEL sentence: act(p(MGI:Gpr64)) -> bp(GOBP:spermatogenesis)\n",
      "Deleted BEL sentence: act(p(MGI:Gstp1)) -| act(p(HGNC:MAPK1))\n",
      "Deleted BEL sentence: act(p(MGI:Gstp1)) -| act(p(HGNC:MAPK3))\n",
      "Deleted BEL sentence: act(p(MGI:Gstp1)) -| act(p(HGNC:MAPK8))\n",
      "Deleted BEL sentence: act(p(HGNC:HELLS)) -| p(MGI:Cdkn1c)\n",
      "Deleted BEL sentence: act(p(HGNC:ITGAV)) -| p(EGID:12096)\n",
      "Deleted BEL sentence: act(p(HGNC:LIPE)) -> p(EGID:103968)\n",
      "Deleted BEL sentence: act(p(HGNC:MMP12)) -> deg(p(MGI:Eln))\n",
      "Deleted BEL sentence: act(p(MGI:\"mt-Co2\")) -> p(HGNC:C1QB)\n",
      "Deleted BEL sentence: act(p(MGI:Niacr1)) -| bp(GOBP:\"macrophage differentiation\")\n",
      "Deleted BEL sentence: act(p(MGI:Niacr1)) -| p(HGNC:ARG2)\n",
      "Deleted BEL sentence: act(p(MGI:Niacr1)) -> p(HGNC:ABCG1)\n",
      "Deleted BEL sentence: act(p(HGNC:SHH)) -| p(EGID:387202)\n",
      "Deleted BEL sentence: act(p(HGNC:CLGN)) -> complex(p(MGI:Adam1a),p(HGNC:ADAM2))\n",
      "Deleted BEL sentence: complex(p(MGI:Atf1),p(HGNC:CREB1)) -> p(HGNC:HSPA5)\n",
      "Deleted BEL sentence: complex(p(MGI:Ccnb1),p(HGNC:CDK2)) -> act(p(HGNC:CDK2))\n",
      "Deleted BEL sentence: complex(p(HGNC:CEBPB),p(HGNC:NFE2L1)) -| p(MGI:Dspp)\n",
      "Deleted BEL sentence: complex(p(MGI:\"H2-T23\"),p(HGNC:INS)) -> bp(GOBP:\"inflammatory response\")\n",
      "Deleted BEL sentence: complex(p(HGNC:IL12A),p(HGNC:IL12B)) -> p(MGI:Gzmg)\n",
      "Deleted BEL sentence: complex(p(HGNC:IL12B),p(HGNC:IL23A)) -> p(MGI:Il22)\n",
      "Deleted BEL sentence: complex(p(HGNC:IL12B),p(HGNC:IL23A)) -> p(MGI:Il22)\n",
      "Deleted BEL sentence: complex(p(MGI:Ins1),p(HGNC:INSR)) -> act(p(HGNC:INSR))\n",
      "Deleted BEL sentence: complex(p(MGI:Irs3),p(HGNC:PTPN11)) -> bp(GOBP:\"MAPK cascade\")\n",
      "Deleted BEL sentence: complex(p(MGI:Irs3),p(HGNC:PTPN11)) -> act(p(HGNC:PTPN11))\n",
      "Deleted BEL sentence: complex(p(HGNC:NFE2L1),p(HGNC:MAFG)) -> p(MGI:\"Hba-a1\")\n",
      "Deleted BEL sentence: complex(p(HGNC:NFE2L1),p(HGNC:MAFG)) -> p(MGI:\"Hbb-b1\")\n",
      "Deleted BEL sentence: complex(p(MGI:Serpinb1b),p(HGNC:CTSG)) -| act(p(HGNC:CTSG))\n",
      "Deleted BEL sentence: act(p(MGI:Cdc42)) -> act(p(HGNC:PRKCZ))\n",
      "Deleted BEL sentence: act(p(EGID:12534)) -> p(HGNC:MYCN,pmod(P))\n",
      "Deleted BEL sentence: act(p(EGID:279561)) -| act(p(HGNC:SLC12A4))\n",
      "Deleted BEL sentence: act(p(MGI:Adrbk1)) -> p(HGNC:SCNN1B,pmod(P))\n",
      "Deleted BEL sentence: act(p(MGI:Adrbk1)) -> act(p(HGNC:SCNN1B))\n",
      "Deleted BEL sentence: act(p(MGI:Adrbk2)) -| act(p(HGNC:OPRK1))\n",
      "Deleted BEL sentence: act(p(HGNC:AKT1)) -| p(MGI:Cdkn1c)\n",
      "Deleted BEL sentence: act(p(HGNC:CDK5)) -> act(p(EGID:18555))\n",
      "Deleted BEL sentence: act(p(HGNC:CDK5)) -> p(EGID:18555,pmod(P))\n",
      "Deleted BEL sentence: act(p(HGNC:EGFR)) -| p(MGI:Wfdc18)\n",
      "Deleted BEL sentence: act(p(HGNC:IKBKE)) -> p(MGI:Ifi203)\n",
      "Deleted BEL sentence: act(p(HGNC:IKBKE)) -> p(MGI:Ifi203)\n",
      "Deleted BEL sentence: act(p(HGNC:IKBKE)) -> p(MGI:Ifit3)\n",
      "Deleted BEL sentence: act(p(HGNC:MAPK8)) -> p(MGI:Ywhaq,pmod(P))\n",
      "Deleted BEL sentence: act(p(HGNC:NEK9)) -| p(MGI:Mup1)\n",
      "Deleted BEL sentence: act(p(HGNC:NEK9)) -| p(MGI:Mup2)\n",
      "Deleted BEL sentence: act(p(MGI:Sik1)) -| act(p(HGNC:CREB1))\n",
      "Deleted BEL sentence: act(p(HGNC:SRC)) -| p(EGID:665646)\n",
      "Deleted BEL sentence: act(p(HGNC:SRC)) -| p(MGI:Cox6c)\n",
      "Deleted BEL sentence: act(p(HGNC:WEE1)) -> p(EGID:12534,pmod(P))\n",
      "Deleted BEL sentence: p(MGI:Ifna1) -> act(p(HGNC:STAT1))\n",
      "Deleted BEL sentence: p(MGI:Ifna1) -> act(p(HGNC:STAT1))\n",
      "Deleted BEL sentence: p(EGID:12765) -> p(HGNC:CXCL10)\n",
      "Deleted BEL sentence: p(EGID:12765) -> p(MGI:Cxcl9)\n",
      "Deleted BEL sentence: p(EGID:12806) -| act(p(HGNC:IRF8))\n",
      "Deleted BEL sentence: p(EGID:19368) -> act(p(HGNC:CASP3))\n",
      "Deleted BEL sentence: p(EGID:19368) -> act(p(HGNC:CASP7))\n",
      "Deleted BEL sentence: p(EGID:19368) -> p(MGI:Gzmb)\n",
      "Deleted BEL sentence: p(HGNC:ADAM19) -> tloc(p(MGI:Nrg1))\n",
      "Deleted BEL sentence: p(HGNC:ADA) -| p(MGI:Ccl7)\n",
      "Deleted BEL sentence: p(HGNC:ADA) -| p(MGI:Ccl7)\n",
      "Deleted BEL sentence: p(HGNC:AK1) -| p(MGI:Gapdh)\n",
      "Deleted BEL sentence: p(MGI:Apoc3) -| act(p(HGNC:LPL))\n",
      "Deleted BEL sentence: p(HGNC:APOE) -> p(MGI:Mir143)\n",
      "Deleted BEL sentence: p(HGNC:APOE) -> p(MGI:Mir145)\n",
      "Deleted BEL sentence: p(HGNC:AREG) -| p(MGI:Cbs)\n",
      "Deleted BEL sentence: p(HGNC:BGN) -> p(EGID:12096)\n",
      "Deleted BEL sentence: p(MGI:Bhlhe41) -> complex(p(HGNC:CEBPB),p(HGNC:HDAC1))\n",
      "Deleted BEL sentence: p(MGI:Bhlhe41) -| act(p(HGNC:CEBPA))\n",
      "Deleted BEL sentence: p(MGI:Bhlhe41) -| act(p(HGNC:CEBPB))\n",
      "Deleted BEL sentence: p(HGNC:BMP2) -> p(EGID:12096)\n",
      "Deleted BEL sentence: p(HGNC:BMP2) -| p(EGID:12096)\n",
      "Deleted BEL sentence: p(HGNC:BMP2) -> p(MGI:Bglap2)\n",
      "Deleted BEL sentence: p(HGNC:BMP2) -| p(MGI:Bglap2)\n",
      "Deleted BEL sentence: p(HGNC:BMP2) -> tloc(p(EGID:12096))\n",
      "Deleted BEL sentence: p(HGNC:BMP2) -> tloc(p(MGI:Bglap2))\n",
      "Deleted BEL sentence: p(MGI:Ccl19) -> act(p(HGNC:JAK3))\n",
      "Deleted BEL sentence: p(MGI:Ccl19) -> p(HGNC:CD40)\n",
      "Deleted BEL sentence: p(MGI:Ccl19) -> p(HGNC:CD86)\n",
      "Deleted BEL sentence: p(MGI:Ccl21a) -> act(p(HGNC:JAK3))\n",
      "Deleted BEL sentence: p(MGI:Cdc42) -> bp(GOBP:\"cell differentiation\")\n",
      "Deleted BEL sentence: p(MGI:Cdc42) -| deg(p(HGNC:CTNNB1))\n",
      "Deleted BEL sentence: p(MGI:Cdkn1c) -| bp(GOBP:\"cell proliferation\")\n",
      "Deleted BEL sentence: p(HGNC:CELA1) -> p(MGI:Eln)\n",
      "Deleted BEL sentence: p(HGNC:CELA1) -> p(MGI:Eln)\n",
      "Deleted BEL sentence: p(HGNC:CHRD) -| act(p(EGID:109899))\n",
      "Deleted BEL sentence: p(HGNC:CHRD) -| p(MGI:Bglap2)\n",
      "Deleted BEL sentence: p(MGI:Clca3) -| p(HGNC:BAX)\n",
      "Deleted BEL sentence: p(MGI:Clca3) -> p(HGNC:BCL2)\n",
      "Deleted BEL sentence: p(MGI:Cops2) -| p(HGNC:CCNE1)\n",
      "Deleted BEL sentence: p(MGI:Cops2) -| p(HGNC:CDKN1A)\n",
      "Deleted BEL sentence: p(MGI:Cops2) -| p(HGNC:TP53)\n",
      "Deleted BEL sentence: p(MGI:Cops2) -> tloc(p(HGNC:NIF3L1))\n",
      "Deleted BEL sentence: p(HGNC:CRY1) -> act(p(EGID:12534))\n",
      "Deleted BEL sentence: p(HGNC:CSF1) -> p(EGID:111785)\n",
      "Deleted BEL sentence: p(HGNC:CSF2) -> act(complex(p(HGNC:CSF2RA),p(MGI:Csf2rb)))\n",
      "Deleted BEL sentence: p(MGI:Cxcl11) -> bp(GOBP:\"T cell proliferation\")\n",
      "Deleted BEL sentence: p(MGI:Cxcl11) -| p(HGNC:COL1A1)\n",
      "Deleted BEL sentence: p(HGNC:CXCL2) -> act(p(EGID:12765))\n",
      "Deleted BEL sentence: p(HGNC:CXCL3) -> act(p(EGID:12765))\n",
      "Deleted BEL sentence: p(HGNC:CXCL3) -> act(p(EGID:12765))\n",
      "Deleted BEL sentence: p(MGI:Defb4) -> act(p(HGNC:TLR4))\n",
      "Deleted BEL sentence: p(HGNC:DLX3) -> p(EGID:109899)\n",
      "Deleted BEL sentence: p(HGNC:DLX3) -> p(MGI:Bglap2)\n",
      "Deleted BEL sentence: p(HGNC:DLX3) -> p(MGI:Esx1)\n",
      "Deleted BEL sentence: p(HGNC:DNAJA1) -| p(EGID:18617)\n",
      "Deleted BEL sentence: p(HGNC:DNAJA1) -| p(EGID:21753)\n",
      "Deleted BEL sentence: p(HGNC:EGF) -| p(EGID:104191)\n",
      "Deleted BEL sentence: p(HGNC:EGF) -| p(MGI:Wfdc18)\n",
      "Deleted BEL sentence: p(HGNC:EGR1) -> p(EGID:319520)\n",
      "Deleted BEL sentence: p(HGNC:FADD) -> p(MGI:Ifna2)\n",
      "Deleted BEL sentence: p(HGNC:FADD) -> p(MGI:Ifna4)\n",
      "Deleted BEL sentence: p(HGNC:FADD) -> p(MGI:Ifna5)\n",
      "Deleted BEL sentence: p(HGNC:FOXA2) -| p(MGI:Abcb1b)\n",
      "Deleted BEL sentence: p(HGNC:FOXM1) -> p(MGI:Ccnb1)\n",
      "Deleted BEL sentence: p(HGNC:GATA4) -> act(p(MGI:\"Nkx2-5\"))\n",
      "Deleted BEL sentence: p(HGNC:CSHL1) -> p(MGI:Serpina3k)\n",
      "Deleted BEL sentence: p(MGI:\"H2-T23\") -> bp(GOBP:\"T cell activation\")\n",
      "Deleted BEL sentence: p(HGNC:HFE) -> p(MGI:Hamp)\n",
      "Deleted BEL sentence: p(HGNC:HIF1A) -> p(MGI:Bnip3l)\n",
      "Deleted BEL sentence: p(HGNC:HIF1A) -> p(MGI:Retnla)\n",
      "Deleted BEL sentence: p(MGI:Hmgb1) -> bp(GOBP:\"neutrophil activation\")\n",
      "Deleted BEL sentence: p(MGI:Hmgb1) -> act(p(HGNC:TLR4))\n",
      "Deleted BEL sentence: p(MGI:Hmgb1) -> act(p(HGNC:AKT1))\n",
      "Deleted BEL sentence: p(MGI:Hmgb1) -> act(p(HGNC:IRAK4))\n",
      "Deleted BEL sentence: p(MGI:Hmgn3) -> p(HGNC:SLC2A2)\n",
      "Deleted BEL sentence: p(MGI:Hmgn3) -> p(HGNC:SLC2A2)\n",
      "Deleted BEL sentence: p(HGNC:HNF1A) -> p(MGI:Il22)\n",
      "Deleted BEL sentence: p(MGI:Hnrnpa1) -> p(HGNC:CCND1)\n",
      "Deleted BEL sentence: p(MGI:Hnrnpa1) -> p(HGNC:MYC)\n",
      "Deleted BEL sentence: p(HGNC:HOXA13) -| p(MGI:Selenbp1)\n",
      "Deleted BEL sentence: p(HGNC:HP) -| tloc(p(MGI:Orm1))\n",
      "Deleted BEL sentence: p(MGI:Hrg) -| bp(GOBP:angiogenesis)\n",
      "Deleted BEL sentence: p(MGI:Hrg) -| bp(GOBP:\"cell proliferation\")\n",
      "Deleted BEL sentence: p(MGI:Hspa1a) -| path(MESHD:Hypertrophy)\n",
      "Deleted BEL sentence: p(MGI:Hspa1b) -> bp(GOBP:\"wound healing\")\n",
      "Deleted BEL sentence: p(MGI:Hspa1b) -| path(MESHD:Hypertrophy)\n",
      "Deleted BEL sentence: p(MGI:Hspa1b) -> p(HGNC:MAPK14)\n",
      "Deleted BEL sentence: p(MGI:Hspa1b) -> tloc(p(HGNC:TNF))\n",
      "Deleted BEL sentence: p(HGNC:HTT) -> p(EGID:15127)\n",
      "Deleted BEL sentence: p(HGNC:ID2) -> p(MGI:\"Hba-a1\")\n",
      "Deleted BEL sentence: p(MGI:Ifna1) -> p(MGI:\"H2-Ab1\")\n",
      "Deleted BEL sentence: p(HGNC:IFNAR1) -> p(MGI:Ifna1)\n",
      "Deleted BEL sentence: p(HGNC:IFNB1) -> p(MGI:Il22)\n",
      "Deleted BEL sentence: p(HGNC:IFNG) -| p(MGI:Ighg2b)\n",
      "Deleted BEL sentence: p(HGNC:IFNG) -| p(EGID:16020)\n",
      "Deleted BEL sentence: p(HGNC:IFNG) -> p(EGID:380793)\n",
      "Deleted BEL sentence: p(HGNC:IFNG) -| p(MGI:Ighg1)\n",
      "Deleted BEL sentence: p(HGNC:IFNG) -> p(EGID:15033)\n",
      "Deleted BEL sentence: p(HGNC:IFNG) -> p(MGI:Gapdh)\n",
      "Deleted BEL sentence: p(HGNC:IFNG) -> p(MGI:\"H2-Eb2\")\n",
      "Deleted BEL sentence: p(HGNC:IFNG) -> p(MGI:\"H2-T23\")\n",
      "Deleted BEL sentence: p(HGNC:IFNG) -> p(MGI:Ifi47)\n",
      "Deleted BEL sentence: p(HGNC:IFNG) -> surf(p(EGID:15033))\n",
      "Deleted BEL sentence: p(HGNC:IFNG) -> surf(p(MGI:\"H2-D1\"))\n",
      "Deleted BEL sentence: p(HGNC:IFNG) -> surf(p(MGI:\"H2-K1\"))\n",
      "Deleted BEL sentence: p(HGNC:IL13) -> p(MGI:Ccl7)\n",
      "Deleted BEL sentence: p(HGNC:IL13) -| p(MGI:Serpina1a)\n",
      "Deleted BEL sentence: p(HGNC:IL13) -> p(EGID:20755)\n",
      "Deleted BEL sentence: p(HGNC:IL13) -> p(EGID:20755)\n",
      "Deleted BEL sentence: p(HGNC:IL13) -> p(MGI:Ccl7)\n",
      "Deleted BEL sentence: p(HGNC:IL13) -> p(MGI:Ccl8)\n",
      "Deleted BEL sentence: p(HGNC:IL13) -> p(MGI:Clca3)\n",
      "Deleted BEL sentence: p(HGNC:IL13) -> p(MGI:Clca3)\n",
      "Deleted BEL sentence: p(HGNC:IL13) -> p(MGI:Muc1)\n",
      "Deleted BEL sentence: p(HGNC:IL13) -| p(MGI:Serpina1a)\n",
      "Deleted BEL sentence: p(HGNC:IL13) -| p(MGI:Serpina1a)\n",
      "Deleted BEL sentence: p(HGNC:IL13) -> p(MGI:Sprr2b)\n",
      "Deleted BEL sentence: p(HGNC:IL13) -> p(MGI:Sprr2b)\n",
      "Deleted BEL sentence: p(HGNC:IL17F) -> p(MGI:Cxcl9)\n",
      "Deleted BEL sentence: p(HGNC:IL1B) -> p(MGI:Saa2)\n",
      "Deleted BEL sentence: p(MGI:Il22) -| bp(GOBP:\"inflammatory response\")\n",
      "Deleted BEL sentence: p(MGI:Il22) -> path(MESHD:Hyperplasia)\n",
      "Deleted BEL sentence: p(MGI:Il22) -> p(MGI:Defb1)\n",
      "Deleted BEL sentence: p(MGI:Il22) -> p(HGNC:IL1B)\n",
      "Deleted BEL sentence: p(MGI:Il22) -> p(HGNC:S100A8)\n",
      "Deleted BEL sentence: p(MGI:Il22) -> p(HGNC:S100A9)\n",
      "Deleted BEL sentence: p(HGNC:IL33) -> p(MGI:Chil3)\n",
      "Deleted BEL sentence: p(MGI:Il3) -> bp(GOBP:\"cell growth\")\n",
      "Deleted BEL sentence: p(MGI:Il3) -| bp(GOBP:\"apoptotic process\")\n",
      "Deleted BEL sentence: p(MGI:Il3) -| bp(GOBP:\"apoptotic process\")\n",
      "Deleted BEL sentence: p(MGI:Il3) -| bp(GOBP:\"apoptotic process\")\n",
      "Deleted BEL sentence: p(MGI:Il3) -> act(p(HGNC:MAPK1))\n",
      "Deleted BEL sentence: p(MGI:Il3) -> act(p(HGNC:MAPK3))\n",
      "Deleted BEL sentence: p(MGI:Il3) -> p(HGNC:MYC)\n",
      "Deleted BEL sentence: p(MGI:Il3) -| p(HGNC:AATK)\n",
      "Deleted BEL sentence: p(MGI:Il3) -> p(HGNC:BIRC5)\n",
      "Deleted BEL sentence: p(MGI:Il3) -| p(HGNC:LCN2)\n",
      "Deleted BEL sentence: p(MGI:Il3) -> p(HGNC:PIM1)\n",
      "Deleted BEL sentence: p(MGI:Il3) -| p(HGNC:ZNF346)\n",
      "Deleted BEL sentence: p(HGNC:IL4R) -> p(MGI:Clca3)\n",
      "Deleted BEL sentence: p(HGNC:IL4) -> p(EGID:20755)\n",
      "Deleted BEL sentence: p(HGNC:IL4) -> p(MGI:Clca3)\n",
      "Deleted BEL sentence: p(HGNC:IL4) -> p(MGI:Sprr2b)\n",
      "Deleted BEL sentence: p(HGNC:IL6) -> p(MGI:Kng1)\n",
      "Deleted BEL sentence: p(HGNC:IL6) -> p(MGI:Serpina3k)\n",
      "Deleted BEL sentence: p(HGNC:IL9) -> p(MGI:Clca3)\n",
      "Deleted BEL sentence: p(HGNC:ING1) -| p(MGI:Ccnb1)\n",
      "Deleted BEL sentence: p(HGNC:IRF4) -| p(EGID:14980)\n",
      "Deleted BEL sentence: p(HGNC:IRF7) -> p(MGI:Ifna1)\n",
      "Deleted BEL sentence: p(HGNC:IRF8) -| p(EGID:14980)\n",
      "Deleted BEL sentence: p(HGNC:IRF8) -| p(MGI:Pmaip1)\n",
      "Deleted BEL sentence: p(HGNC:IRF9) -> p(MGI:Ifna1)\n",
      "Deleted BEL sentence: p(MGI:Irs3) -| bp(GOBP:mitosis)\n",
      "Deleted BEL sentence: p(MGI:Irs3) -> bp(GOBP:\"apoptotic process\")\n",
      "Deleted BEL sentence: p(MGI:Irs3) -> p(HGNC:EGR1)\n",
      "Deleted BEL sentence: p(MGI:Irs3) -| p(HGNC:IRS2)\n",
      "Deleted BEL sentence: p(MGI:Irs4) -| p(HGNC:IRS2)\n",
      "Deleted BEL sentence: p(HGNC:ITGAV) -> p(MGI:Mcpt1)\n",
      "Deleted BEL sentence: p(HGNC:ITGAV) -> tloc(p(MGI:Mcpt1))\n",
      "Deleted BEL sentence: p(HGNC:JDP2) -> p(MGI:Acp5)\n",
      "Deleted BEL sentence: p(HGNC:JUN) -| act(p(MGI:Casp12))\n",
      "Deleted BEL sentence: p(HGNC:JUN) -| act(p(MGI:Casp12))\n",
      "Deleted BEL sentence: p(HGNC:KEAP1) -| p(EGID:110309)\n",
      "Deleted BEL sentence: p(HGNC:KHDRBS1) -| p(EGID:12096)\n",
      "Deleted BEL sentence: p(HGNC:KHDRBS1) -| p(MGI:Bglap2)\n",
      "Deleted BEL sentence: p(MGI:Klk1b4) -> p(HGNC:CHN1)\n",
      "Deleted BEL sentence: p(MGI:Klk1b4) -> p(HGNC:CHN2)\n",
      "Deleted BEL sentence: p(MGI:Ly6a) -| bp(GOBP:\"T cell proliferation\")\n",
      "Deleted BEL sentence: p(MGI:Ly6a) -> tloc(p(HGNC:IL4))\n",
      "Deleted BEL sentence: p(HGNC:MAB21L1) -> p(MGI:Foxe3)\n",
      "Deleted BEL sentence: p(HGNC:MAP2K7) -> act(complex(p(EGID:12534),p(MGI:Ccnb1)))\n",
      "Deleted BEL sentence: p(HGNC:MAP2K7) -> p(EGID:12534)\n",
      "Deleted BEL sentence: p(MGI:Map3k7) -> p(HGNC:LOX)\n",
      "Deleted BEL sentence: p(MGI:Map3k7) -> p(HGNC:THBS1)\n",
      "Deleted BEL sentence: p(MGI:Map3k7) -> p(HGNC:TIMP3)\n",
      "Deleted BEL sentence: p(MGI:Map3k7) -> p(HGNC:VCL)\n",
      "Deleted BEL sentence: p(HGNC:MMP12) -| p(MGI:Serpina1a)\n",
      "Deleted BEL sentence: p(HGNC:MMP12) -| p(MGI:Serpina1b)\n",
      "Deleted BEL sentence: p(HGNC:MMP7) -> deg(p(MGI:Eln))\n",
      "Deleted BEL sentence: p(MGI:Mt1) -| bp(GOBP:\"response to oxidative stress\")\n",
      "Deleted BEL sentence: p(MGI:Mt1) -> p(MGI:Serpina3k)\n",
      "Deleted BEL sentence: p(MGI:Mt1) -| p(HGNC:TKT)\n",
      "Deleted BEL sentence: p(MGI:Mt1) -| p(MGI:Vnn3)\n",
      "Deleted BEL sentence: p(MGI:Muc4) -> bp(GOBP:\"cell growth\")\n",
      "Deleted BEL sentence: p(MGI:Muc4) -> act(p(HGNC:MAPK1))\n",
      "Deleted BEL sentence: p(MGI:Muc4) -> act(p(HGNC:MAPK3))\n",
      "Deleted BEL sentence: p(MGI:Muc4) -> p(HGNC:ERBB2)\n",
      "Deleted BEL sentence: p(MGI:Muc4) -> p(HGNC:ERBB2,pmod(P))\n",
      "Deleted BEL sentence: p(HGNC:NCOR2) -| p(EGID:216850)\n",
      "Deleted BEL sentence: p(HGNC:NFE2) -> p(MGI:Hsd3b6)\n",
      "Deleted BEL sentence: p(HGNC:NGEF) -| act(p(MGI:Cdc42))\n",
      "Deleted BEL sentence: p(HGNC:NIF3L1) -> act(p(MGI:Cops2))\n",
      "Deleted BEL sentence: p(MGI:\"Nkx2-1\") -> bp(GOBP:\"inflammatory response\")\n",
      "Deleted BEL sentence: p(MGI:\"Nkx2-1\") -> path(MESHD:Emphysema)\n",
      "Deleted BEL sentence: p(MGI:\"Nkx2-5\") -> act(p(HGNC:GATA4))\n",
      "Deleted BEL sentence: p(HGNC:NOS2) -> p(EGID:20755)\n",
      "Deleted BEL sentence: p(HGNC:NOS3) -| p(MGI:Madcam1)\n",
      "Deleted BEL sentence: p(HGNC:NOS3) -| p(MGI:Cyp2c29)\n",
      "Deleted BEL sentence: p(MGI:Nrg1) -> p(HGNC:ERRFI1)\n",
      "Deleted BEL sentence: p(HGNC:ORMDL3) -> p(MGI:Cxcl11)\n",
      "Deleted BEL sentence: p(HGNC:PAX9) -| p(MGI:Krt10)\n",
      "Deleted BEL sentence: p(HGNC:PAX9) -| p(MGI:Krt2)\n",
      "Deleted BEL sentence: p(MGI:Pde4d) -> bp(GOBP:\"neutrophil chemotaxis\")\n",
      "Deleted BEL sentence: p(MGI:Pde4d) -> p(HGNC:TNF)\n",
      "Deleted BEL sentence: p(HGNC:PDE7A) -> p(MGI:Il22)\n",
      "Deleted BEL sentence: p(HGNC:PDPK1) -| complex(p(HGNC:PDPK1),p(MGI:Ywhaq))\n",
      "Deleted BEL sentence: p(HGNC:PDPK1) -| complex(p(HGNC:PDPK1),p(MGI:Ywhaq))\n",
      "Deleted BEL sentence: p(HGNC:PDPK1) -> complex(p(HGNC:PDPK1),p(MGI:Ywhaq))\n",
      "Deleted BEL sentence: p(HGNC:PHOX2B) -| p(MGI:\"Nkx6-1\")\n",
      "Deleted BEL sentence: p(HGNC:PHOX2B) -| p(MGI:\"Nkx6-2\")\n",
      "Deleted BEL sentence: p(HGNC:PLA2G6) -> act(p(MGI:Ighm))\n",
      "Deleted BEL sentence: p(HGNC:PLAA) -> p(MGI:\"mt-Co2\")\n",
      "Deleted BEL sentence: p(MGI:Pln) -| p(HGNC:FKBP1A)\n",
      "Deleted BEL sentence: p(MGI:Pln) -| p(HGNC:RYR1)\n",
      "Deleted BEL sentence: p(MGI:Pmaip1) -> act(p(HGNC:BAK1))\n",
      "Deleted BEL sentence: p(MGI:Pmaip1) -> act(p(HGNC:CASP7))\n",
      "Deleted BEL sentence: p(HGNC:POR) -| p(MGI:Cyp2c29)\n",
      "Deleted BEL sentence: p(HGNC:POR) -| p(MGI:Cyp3a11)\n",
      "Deleted BEL sentence: p(HGNC:POR) -| p(MGI:Scd2)\n",
      "Deleted BEL sentence: p(HGNC:PPP1R15A) -> act(p(MGI:Ppp1cc))\n",
      "Deleted BEL sentence: p(HGNC:PPP1R15A) -> p(MGI:Ppp1cc)\n",
      "Deleted BEL sentence: p(HGNC:PRDM16) -| p(MGI:Serpina3k)\n",
      "Deleted BEL sentence: p(MGI:Retnla) -> bp(GOBP:angiogenesis)\n",
      "Deleted BEL sentence: p(MGI:Retnla) -> bp(GOBP:\"cell proliferation\")\n",
      "Deleted BEL sentence: p(HGNC:RNF2) -| p(MGI:Pmaip1)\n",
      "Deleted BEL sentence: p(HGNC:RXRG) -> p(MGI:Cyp2c29)\n",
      "Deleted BEL sentence: p(MGI:Sepp1) -> bp(GOBP:\"cell growth\")\n",
      "Deleted BEL sentence: p(MGI:Sepp1) -| path(MESHD:Ataxia)\n",
      "Deleted BEL sentence: p(MGI:Serpina1b) -| act(p(HGNC:PLG))\n",
      "Deleted BEL sentence: p(HGNC:SERPINE1) -| p(MGI:Klrb1c)\n",
      "Deleted BEL sentence: p(MGI:Sik1,pmod(P)) -> tloc(p(MGI:Sik1))\n",
      "Deleted BEL sentence: p(MGI:Sik1) -| act(p(HGNC:CREB1))\n",
      "Deleted BEL sentence: p(HGNC:SOCS2) -| p(MGI:Ifna1)\n",
      "Deleted BEL sentence: p(HGNC:SOCS2) -> p(MGI:Mup1)\n",
      "Deleted BEL sentence: p(HGNC:SOCS3) -| p(MGI:Ifna1)\n",
      "Deleted BEL sentence: p(HGNC:SOD2) -> p(MGI:Hspa1a)\n",
      "Deleted BEL sentence: p(HGNC:SOD2) -| p(MGI:Mt1)\n",
      "Deleted BEL sentence: p(HGNC:SOD2) -| p(MGI:Mt2)\n",
      "Deleted BEL sentence: p(HGNC:SOX15) -| p(MGI:Hrc)\n",
      "Deleted BEL sentence: p(HGNC:SPP1) -> p(EGID:12096)\n",
      "Deleted BEL sentence: p(HGNC:SPP1) -| p(MGI:\"mt-Co1\")\n",
      "Deleted BEL sentence: p(HGNC:TAF4B) -> p(MGI:Bmp8b)\n",
      "Deleted BEL sentence: p(HGNC:TBX21) -> p(MGI:Ighg2b)\n",
      "Deleted BEL sentence: p(HGNC:TBX21) -| p(EGID:16020)\n",
      "Deleted BEL sentence: p(HGNC:TBX21) -> p(EGID:380793)\n",
      "Deleted BEL sentence: p(HGNC:TBX21) -| p(MGI:Ighg1)\n",
      "Deleted BEL sentence: p(HGNC:TBX21) -> p(EGID:380793)\n",
      "Deleted BEL sentence: p(HGNC:TGFB1) -| p(EGID:387222)\n",
      "Deleted BEL sentence: p(HGNC:TGFB1) -| p(EGID:387222)\n",
      "Deleted BEL sentence: p(HGNC:TGFB1) -| p(EGID:387223)\n",
      "Deleted BEL sentence: p(HGNC:TGFB1) -| p(EGID:387223)\n",
      "Deleted BEL sentence: p(HGNC:TGFB1) -| p(EGID:387224)\n",
      "Deleted BEL sentence: p(HGNC:TGFB1) -| p(EGID:387224)\n",
      "Deleted BEL sentence: p(HGNC:TGFB1) -| p(EGID:69717)\n",
      "Deleted BEL sentence: p(HGNC:TGFB1) -> p(MGI:Mcpt1)\n",
      "Deleted BEL sentence: p(HGNC:TLR2) -| p(EGID:12765)\n",
      "Deleted BEL sentence: p(HGNC:TLR4) -| deg(p(MGI:Eln))\n",
      "Deleted BEL sentence: p(HGNC:TLR7) -> p(MGI:Ifna2)\n",
      "Deleted BEL sentence: p(HGNC:TNF) -| p(EGID:387223)\n",
      "Deleted BEL sentence: p(HGNC:TNF) -> p(MGI:Ccl6)\n",
      "Deleted BEL sentence: p(HGNC:TWSG1) -| act(p(EGID:109899))\n",
      "Deleted BEL sentence: p(HGNC:TWSG1) -| p(MGI:Bglap2)\n",
      "Deleted BEL sentence: p(HGNC:TYROBP) -| p(MGI:Icosl)\n",
      "Deleted BEL sentence: p(HGNC:WNT3A) -> p(EGID:12096)\n",
      "Deleted BEL sentence: p(HGNC:XBP1) -> p(EGID:13216)\n",
      "Deleted BEL sentence: p(HGNC:XBP1) -> p(EGID:13238)\n",
      "Deleted BEL sentence: p(HGNC:XBP1) -> p(EGID:13239)\n",
      "Deleted BEL sentence: p(HGNC:XBP1) -> p(MGI:Lyz2)\n",
      "Deleted BEL sentence: p(HGNC:ZFPM1) -> p(MGI:\"Hba-a1\")\n",
      "Deleted BEL sentence: p(HGNC:ZFPM1) -> p(MGI:\"Hbb-b1\")\n",
      "Deleted BEL sentence: p(HGNC:ZFPM1) -> p(MGI:Srgn)\n",
      "Deleted BEL sentence: p(EGID:387243) -| p(HGNC:NFAT5)\n",
      "Deleted BEL sentence: p(EGID:387243) -| act(p(HGNC:NFAT5))\n",
      "Deleted BEL sentence: p(EGID:751531) -| p(HGNC:NFAT5)\n",
      "Deleted BEL sentence: p(EGID:751531) -| act(p(HGNC:NFAT5))\n",
      "Deleted BEL sentence: p(MGI:Mir155) -| p(HGNC:IL10)\n",
      "Deleted BEL sentence: p(MGI:Mir155) -| p(HGNC:IL4)\n",
      "Deleted BEL sentence: p(MGI:Mir155) -| p(HGNC:IL5)\n",
      "Deleted BEL sentence: act(p(HGNC:AHR)) -> p(MGI:Il22)\n",
      "Deleted BEL sentence: act(p(HGNC:AR)) -> p(MGI:Pbsn)\n",
      "Deleted BEL sentence: act(p(MGI:Atf1)) -| bp(GOBP:\"apoptotic process\")\n",
      "Deleted BEL sentence: act(p(HGNC:ATF4)) -> p(MGI:Bglap2)\n",
      "Deleted BEL sentence: act(p(HGNC:BCL6)) -| p(MGI:Ccl6)\n",
      "Deleted BEL sentence: act(p(HGNC:BCL6)) -| p(MGI:Ccl7)\n",
      "Deleted BEL sentence: act(p(MGI:Bhlhe41)) -| p(HGNC:CEBPA)\n",
      "Deleted BEL sentence: act(p(MGI:Bhlhe41)) -| p(HGNC:PPARG)\n",
      "Deleted BEL sentence: act(p(HGNC:CEBPB)) -| p(MGI:Dspp)\n",
      "Deleted BEL sentence: act(p(HGNC:CREB1)) -> p(MGI:\"H2-Eb2\")\n",
      "Deleted BEL sentence: act(p(HGNC:CREB1)) -> p(MGI:Pvrl2)\n",
      "Deleted BEL sentence: act(p(MGI:Cux1)) -| p(HGNC:RUNX2)\n",
      "Deleted BEL sentence: act(p(HGNC:E2F1)) -> p(MGI:Hrk)\n",
      "Deleted BEL sentence: act(p(HGNC:E2F1)) -> p(MGI:Pmaip1)\n",
      "Deleted BEL sentence: act(p(HGNC:E2F4)) -> p(MGI:Ccnb1)\n",
      "Deleted BEL sentence: act(p(MGI:Eno1)) -> bp(GOBP:\"cell death\")\n",
      "Deleted BEL sentence: act(p(MGI:Eno1)) -| p(HGNC:MYC)\n",
      "Deleted BEL sentence: act(p(MGI:Eno1)) -| p(HGNC:MYC)\n",
      "Deleted BEL sentence: act(p(HGNC:EOMES)) -> p(MGI:Gzmb)\n",
      "Deleted BEL sentence: act(p(HGNC:EPAS1)) -> p(MGI:Irs3)\n",
      "Deleted BEL sentence: act(p(HGNC:ESR1)) -| p(EGID:100124672)\n",
      "Deleted BEL sentence: act(p(MGI:Esrra)) -> bp(GOBP:\"fatty acid oxidation\")\n",
      "Deleted BEL sentence: act(p(HGNC:FLI1)) -| p(EGID:15127)\n",
      "Deleted BEL sentence: act(p(HGNC:FOS)) -> p(MGI:Pvrl2)\n",
      "Deleted BEL sentence: act(p(HGNC:FOXA2)) -> p(MGI:Gcg)\n",
      "Deleted BEL sentence: act(p(HGNC:FOXO1)) -> p(MGI:Apoc3)\n",
      "Deleted BEL sentence: act(p(MGI:Foxp2)) -| p(HGNC:SCGB1A1)\n",
      "Deleted BEL sentence: act(p(MGI:Foxp2)) -| p(HGNC:SFTPC)\n",
      "Deleted BEL sentence: act(p(HGNC:GABPB2)) -> p(MGI:\"mt-Co2\")\n",
      "Deleted BEL sentence: act(p(HGNC:GTF2IRD1)) -> p(EGID:111507)\n",
      "Deleted BEL sentence: act(p(HGNC:GTF2IRD1)) -| p(EGID:111507)\n",
      "Deleted BEL sentence: act(p(HGNC:HIF1A)) -> p(EGID:387206)\n",
      "Deleted BEL sentence: act(p(HGNC:HOXC13)) -> p(MGI:Stfa3)\n",
      "Deleted BEL sentence: act(p(HGNC:IRF1)) -> p(MGI:Ifi47)\n",
      "Deleted BEL sentence: act(p(HGNC:JDP2)) -| p(MGI:Fus)\n",
      "Deleted BEL sentence: act(p(HGNC:JUN)) -> p(EGID:12534)\n",
      "Deleted BEL sentence: act(p(HGNC:JUN)) -> p(MGI:Pvrl2)\n",
      "Deleted BEL sentence: act(p(HGNC:KLF5)) -> act(p(EGID:12534))\n",
      "Deleted BEL sentence: act(p(HGNC:KLF5)) -> p(EGID:12534)\n",
      "Deleted BEL sentence: act(p(HGNC:KLF5)) -> p(MGI:Ccnb1)\n",
      "Deleted BEL sentence: act(p(HGNC:LHX9)) -> p(MGI:Sf1)\n",
      "Deleted BEL sentence: act(p(HGNC:LMX1A)) -> p(MGI:Ins1)\n",
      "Deleted BEL sentence: act(p(HGNC:MAFG)) -> p(MGI:\"Hba-a1\")\n",
      "Deleted BEL sentence: act(p(HGNC:MAFG)) -> p(MGI:\"Hbb-b1\")\n",
      "Deleted BEL sentence: act(p(HGNC:MAZ)) -> p(MGI:Ins1)\n",
      "Deleted BEL sentence: act(p(HGNC:MITF)) -> p(MGI:Tpsab1)\n",
      "Deleted BEL sentence: act(p(MGI:Msx3)) -| p(HGNC:MSX1)\n",
      "Deleted BEL sentence: act(p(HGNC:MTF1)) -> p(MGI:Mt1)\n",
      "Deleted BEL sentence: act(p(HGNC:MTF1)) -> p(MGI:Mt1)\n",
      "Deleted BEL sentence: act(p(HGNC:MYC)) -> p(EGID:19368)\n",
      "Deleted BEL sentence: act(p(HGNC:NFE2L1)) -| p(MGI:Cyp4a10)\n",
      "Deleted BEL sentence: act(p(HGNC:NFE2L1)) -| p(MGI:Cyp4a14)\n",
      "Deleted BEL sentence: act(p(HGNC:NFE2L1)) -| p(MGI:Dspp)\n",
      "Deleted BEL sentence: act(p(HGNC:NFE2L2)) -> p(EGID:110309)\n",
      "Deleted BEL sentence: act(p(HGNC:NFIA)) -> act(p(MGI:\"Nkx2-1\"))\n",
      "Deleted BEL sentence: act(p(HGNC:NFIB)) -> act(p(MGI:\"Nkx2-1\"))\n",
      "Deleted BEL sentence: act(p(MGI:\"Nkx2-1\")) -> p(HGNC:SFTPD)\n",
      "Deleted BEL sentence: act(p(MGI:\"Nkx2-5\")) -> p(HGNC:ADORA1)\n",
      "Deleted BEL sentence: act(p(MGI:\"Nkx2-5\")) -> p(HGNC:PITX2)\n",
      "Deleted BEL sentence: act(p(MGI:\"Nkx2-5\")) -> p(HGNC:XIRP1)\n",
      "Deleted BEL sentence: act(p(HGNC:NR1I3)) -| p(MGI:Slco1a1)\n",
      "Deleted BEL sentence: act(p(HGNC:NR1I3)) -> p(MGI:Sult2a1)\n",
      "Deleted BEL sentence: act(p(HGNC:NR4A2)) -> p(MGI:Cdkn1c)\n",
      "Deleted BEL sentence: act(p(HGNC:PAX6)) -| p(MGI:\"Nkx2-1\")\n",
      "Deleted BEL sentence: act(p(HGNC:POU3F4)) -> p(MGI:Gcg)\n",
      "Deleted BEL sentence: act(p(HGNC:PPARA)) -| p(EGID:20493)\n",
      "Deleted BEL sentence: act(p(HGNC:PPARA)) -| p(MGI:Apoc3)\n",
      "Deleted BEL sentence: act(p(HGNC:PPARA)) -| p(MGI:Eif1)\n",
      "Deleted BEL sentence: act(p(HGNC:PPARA)) -| p(MGI:Slco1a1)\n",
      "Deleted BEL sentence: act(p(HGNC:PPARA)) -| p(MGI:Slco1a4)\n",
      "Deleted BEL sentence: act(p(HGNC:PPARD)) -> p(EGID:11520)\n",
      "Deleted BEL sentence: act(p(HGNC:PPARG)) -> p(MGI:Flg)\n",
      "Deleted BEL sentence: act(p(HGNC:PPARG)) -> p(MGI:Ivl)\n",
      "Deleted BEL sentence: act(p(HGNC:PPARG)) -> p(MGI:Lor)\n",
      "Deleted BEL sentence: act(p(HGNC:RB1)) -> p(EGID:12096)\n",
      "Deleted BEL sentence: act(p(HGNC:RB1)) -> p(MGI:Bglap2)\n",
      "Deleted BEL sentence: act(p(HGNC:RB1)) -> tloc(p(EGID:12096))\n",
      "Deleted BEL sentence: act(p(HGNC:RB1)) -> tloc(p(MGI:Bglap2))\n",
      "Deleted BEL sentence: act(p(HGNC:RELB)) -> p(MGI:Ccl19)\n",
      "Deleted BEL sentence: act(p(HGNC:RELB)) -> p(MGI:Ccl21a)\n",
      "Deleted BEL sentence: act(p(HGNC:RUNX2)) -> p(MGI:Bglap2)\n",
      "Deleted BEL sentence: act(p(HGNC:SATB2)) -> p(MGI:Ighm)\n",
      "Deleted BEL sentence: act(p(HGNC:SMAD1)) -> p(MGI:\"Nkx2-5\")\n",
      "Deleted BEL sentence: act(p(HGNC:SMAD4)) -> p(MGI:\"Nkx2-5\")\n",
      "Deleted BEL sentence: act(p(HGNC:SMARCA2)) -> p(MGI:Pbsn)\n",
      "Deleted BEL sentence: act(p(HGNC:SMARCA4)) -> p(MGI:Pbsn)\n",
      "Deleted BEL sentence: act(p(HGNC:SPI1)) -| p(MGI:\"Hbb-b2\")\n",
      "Deleted BEL sentence: act(p(HGNC:STAT4)) -> p(MGI:Ccl6)\n",
      "Deleted BEL sentence: act(p(HGNC:TBX21)) -> act(p(MGI:Gzmb))\n",
      "Deleted BEL sentence: act(p(MGI:Tcf7)) -> p(HGNC:RUNX2)\n",
      "Deleted BEL sentence: act(p(HGNC:TP53)) -> p(MGI:Pmaip1)\n",
      "Deleted BEL sentence: act(p(HGNC:TP53)) -> p(MGI:Pmaip1)\n",
      "Deleted BEL sentence: act(p(MGI:Ttf1)) -> p(HGNC:SFTPD)\n",
      "Deleted BEL sentence: act(p(HGNC:USF1)) -> p(MGI:Mt1)\n",
      "Deleted BEL sentence: act(p(HGNC:USF1)) -> p(MGI:Mt1)\n",
      "Deleted BEL sentence: act(p(HGNC:USF1)) -> p(MGI:Prnd)\n",
      "Deleted BEL sentence: act(p(HGNC:WT1)) -> p(MGI:Sf1)\n",
      "Deleted BEL sentence: act(p(HGNC:YBX3)) -| p(MGI:Prm1)\n",
      "Deleted BEL sentence: act(p(HGNC:YBX3)) -| p(MGI:Prm2)\n",
      "Deleted BEL sentence: act(p(MGI:Tlr9 )) -| bp(GOBP:\"regulatory T cell differentiation\")\n",
      "Deleted BEL sentence: act(p(MGI:Tlr9 )) -> bp(GOBP:\"apoptotic process\")\n",
      "Deleted BEL sentence: bp(GOBP:\"response to tumor cell\") -| p(MGI:Tnfsf9)\n",
      "Deleted BEL sentence: act(p(HGNC:TLR9)) -> p(MGI:Tnfsf9)\n",
      "Deleted BEL sentence: act(p(MGI:Klrk1)) -> tloc(p(HGNC:IFNG))\n",
      "Deleted BEL sentence: p(HGNC:S100A9) -> p(MGI:Nfkb1, pmod(P,S,536))\n",
      "Deleted BEL sentence: p(HGNC:S100A8) -> p(MGI:Nfkb1, pmod(P,S,536))\n",
      "Deleted BEL sentence: p(HGNC:IL6) -| p(MGI:Il1r1 )\n",
      "Deleted BEL sentence: p(HGNC:CSF3) -> p(MGI:Il1r1 )\n",
      "Deleted BEL sentence: p(HGNC:CSF2) -> p(MGI:Il1r1 )\n",
      "Deleted BEL sentence: act(p(MGI:Hif1a )) -| a(CHEBI:\"reactive oxygen species\")\n",
      "Deleted BEL sentence: act(p(HGNC:HIF1A)) -> p(MGI:Emr1)\n",
      "Deleted BEL sentence: p(HGNC:IL1B) -| p(MGI:Ly6c1)\n",
      "Deleted BEL sentence: p(HGNC:IL1B) -| p(MGI:Klrk1)\n",
      "Deleted BEL sentence: p(HGNC:IL1B) -| p(MGI:Klrk1)\n",
      "Deleted BEL sentence: path(MESHD:\"Breast Neoplasms\") -| p(MGI:Klrk1)\n",
      "Deleted BEL sentence: bp(GOBP:\"response to tumor cell\") -| p(MGI:Mir223)\n",
      "Deleted BEL sentence: p(HGNC:PTGS2) -| p(MGI:Mir223)\n",
      "Deleted BEL sentence: p(MGI:Mir223) -| p(HGNC:MEF2C)\n",
      "Deleted BEL sentence: bp(GOBP:\"response to tumor cell\") -| p(MGI:Mir20a)\n",
      "Deleted BEL sentence: bp(GOBP:\"response to tumor cell\") -| p(MGI:Mir17)\n",
      "Deleted BEL sentence: bp(GOBP:\"response to tumor cell\") -| p(MGI:Mir17)\n",
      "Deleted BEL sentence: bp(GOBP:\"response to tumor cell\") -| p(MGI:Mir20a)\n",
      "Deleted BEL sentence: p(MGI:Mir17) -| p(HGNC:STAT3)\n",
      "Deleted BEL sentence: p(MGI:Mir20a) -| p(HGNC:STAT3)\n",
      "Deleted BEL sentence: p(MGI:Mir17) -| p(HGNC:STAT3)\n",
      "Deleted BEL sentence: p(MGI:Mir20a) -| p(HGNC:STAT3)\n",
      "Deleted BEL sentence: p(MGI:Mir17) -| p(HGNC:STAT3)\n",
      "Deleted BEL sentence: p(MGI:Mir20a) -| p(HGNC:STAT3)\n",
      "Deleted BEL sentence: p(MGI:Mir20a) -| p(HGNC:CYBB)\n",
      "Deleted BEL sentence: p(MGI:Mir20a) -| p(HGNC:NCF1)\n",
      "Deleted BEL sentence: p(MGI:Mir17) -| p(HGNC:CYBB)\n",
      "Deleted BEL sentence: p(MGI:Mir17) -| p(HGNC:NCF1)\n",
      "Deleted BEL sentence: complex(p(MGI:Mir20a),act(p(HGNC:STAT3))) -| p(HGNC:STAT3)\n",
      "Deleted BEL sentence: complex(p(MGI:Mir17),act(p(HGNC:STAT3))) -| p(HGNC:STAT3)\n",
      "Deleted BEL sentence: p(MGI:Mir17) -| a(CHEBI:\"reactive oxygen species\")\n",
      "Deleted BEL sentence: p(MGI:Mir17) -| a(CHEBI:dioxidaniumyl)\n",
      "Deleted BEL sentence: p(MGI:Mir20a) -| a(CHEBI:dioxidaniumyl)\n",
      "Deleted BEL sentence: p(MGI:Mir20a) -| a(CHEBI:\"reactive oxygen species\")\n",
      "Deleted BEL sentence: bp(GOBP:\"response to tumor cell\") -| p(MGI:Mir17)\n",
      "Deleted BEL sentence: bp(GOBP:\"response to tumor cell\") -| p(MGI:Mir20a)\n",
      "Deleted BEL sentence: bp(GOBP:\"response to tumor cell\") -| p(MGI:Mir17)\n",
      "Deleted BEL sentence: bp(GOBP:\"response to tumor cell\") -| p(MGI:Mir20a)\n",
      "Deleted BEL sentence: p(MGI:Mir17) -> a(CHEBI:\"reactive oxygen species\")\n",
      "Deleted BEL sentence: p(MGI:Mir17) -> a(CHEBI:dioxidaniumyl)\n",
      "Deleted BEL sentence: p(MGI:Mir20a) -> a(CHEBI:\"reactive oxygen species\")\n",
      "Deleted BEL sentence: p(MGI:Mir20a) -> a(CHEBI:dioxidaniumyl)\n",
      "Deleted BEL sentence: bp(GOBP:\"response to tumor cell\") -> p(MGI:Mir494)\n",
      "Deleted BEL sentence: bp(GOBP:\"response to tumor cell\") -> p(MGI:Mir494)\n",
      "Deleted BEL sentence: bp(GOBP:\"response to tumor cell\") -> p(MGI:Mir494)\n",
      "Deleted BEL sentence: p(HGNC:TGFB1) -> p(MGI:Mir494)\n",
      "Deleted BEL sentence: p(HGNC:TGFB1) -> p(MGI:Mir494)\n",
      "Deleted BEL sentence: p(HGNC:TGFB1) -> p(MGI:Mir494)\n",
      "Deleted BEL sentence: p(HGNC:TGFB1) -> p(MGI:Mir494)\n",
      "Deleted BEL sentence: p(MGI:Mir494) -> p(HGNC:MMP14)\n",
      "Deleted BEL sentence: p(MGI:Mir494) -> p(HGNC:MMP13)\n",
      "Deleted BEL sentence: p(MGI:Mir494) -> p(HGNC:TINAGL1)\n",
      "Deleted BEL sentence: p(MGI:Mir494) -> p(HGNC:MMP2)\n",
      "Deleted BEL sentence: p(MGI:Mir494) -| act(p(HGNC:ARG1))\n",
      "Deleted BEL sentence: p(MGI:Mir494) -| p(HGNC:PTEN)\n",
      "Deleted BEL sentence: p(HGNC:TGFB1) -> p(MGI:Mir494)\n",
      "Deleted BEL sentence: p(MGI:Mir494) -| p(HGNC:PTEN)\n",
      "Deleted BEL sentence: bp(GOBP:\"response to tumor cell\") -| complex(p(MGI:Csl),p(HGNC:RBPJ))\n",
      "Deleted BEL sentence: bp(GOBP:\"response to tumor cell\") -> p(MGI:Mir21)\n",
      "Deleted BEL sentence: bp(GOBP:\"response to tumor cell\") -> p(MGI:Mir155)\n",
      "Deleted BEL sentence: bp(GOBP:\"response to tumor cell\") -> p(MGI:Mir155)\n",
      "Deleted BEL sentence: bp(GOBP:\"response to tumor cell\") -> p(MGI:Mir21)\n",
      "Deleted BEL sentence: p(MGI:Cyp2a4) -| act(p(HGNC:STAT3))\n",
      "Deleted BEL sentence: p(MGI:Cyp2a4) -| act(p(HGNC:STAT3))\n",
      "Deleted BEL sentence: complex(p(MGI:Itgav ),p(HGNC:ITGB1)) -> path(MESHD:\"Neovascularization, Pathologic\")\n",
      "Deleted BEL sentence: complex(p(MGI:Itgav ),p(HGNC:ITGB1)) -> bp(GOBP:\"cell adhesion\")\n",
      "Deleted BEL sentence: complex(p(MGI:Itgav ),p(HGNC:ITGB1)) -> bp(GOBP:\"macrophage chemotaxis\")\n",
      "Deleted BEL sentence: complex(p(MGI:Itgav ),p(HGNC:ITGB1)) -> bp(GOBP:\"blood vessel development\")\n",
      "Deleted BEL sentence: p(MGI:Mir190) -| p(HGNC:TIMP3)\n",
      "Deleted BEL sentence: p(MGI:Mir190) -| p(HGNC:TGFA)\n",
      "Deleted BEL sentence: p(MGI:Mir190) -> p(HGNC:AMOT)\n",
      "Deleted BEL sentence: p(MGI:Mir190) -> p(HGNC:EPHA5)\n",
      "Deleted BEL sentence: p(MGI:Mir190) -| p(HGNC:TGFA)\n",
      "Word in BEL converted to unknown: GOBP:mitosis\n",
      "Word in BEL converted to unknown: GOBP:glycolysis\n",
      "Word in BEL converted to unknown: HGNC:IL8\n",
      "Word in BEL converted to unknown: GOBP:\"patterning of blood vessels\"\n",
      "Word in BEL converted to unknown: HGNC:MRE11A\n",
      "Word in BEL converted to unknown: HGNC:IL8\n",
      "Word in BEL converted to unknown: GOBP:\"patterning of blood vessels\"\n",
      "Word in BEL converted to unknown: HGNC:IL8\n",
      "Word in BEL converted to unknown: HGNC:IL8\n",
      "Word in BEL converted to unknown: GOBP:\"Wnt receptor signaling pathway\"\n",
      "Word in BEL converted to unknown: GOBP:\"Wnt receptor signaling pathway\"\n",
      "Word in BEL converted to unknown: CHEBI:octreotide\n",
      "Word in BEL converted to unknown: CHEBI:octreotide\n",
      "Word in BEL converted to unknown: CHEBI:octreotide\n",
      "Word in BEL converted to unknown: CHEBI:\"palmitic acid\"\n",
      "Word in BEL converted to unknown: GOBP:mitosis\n",
      "Word in BEL converted to unknown: GOBP:mitosis\n",
      "Word in BEL converted to unknown: GOBP:\"Wnt receptor signaling pathway\"\n",
      "Word in BEL converted to unknown: GOBP:\"Wnt receptor signaling pathway\"\n",
      "Word in BEL converted to unknown: HGNC:IL8\n",
      "Word in BEL converted to unknown: CHEBI:\"cholesterol ester\"\n",
      "Word in BEL converted to unknown: HGNC:IL8\n",
      "Word in BEL converted to unknown: CHEBI:\"palmitoleic acid\"\n",
      "Word in BEL converted to unknown: GOBP:glycolysis\n",
      "Word in BEL converted to unknown: GOBP:\"patterning of blood vessels\"\n",
      "Word in BEL converted to unknown: HGNC:PARK2\n",
      "Word in BEL converted to unknown: HGNC:PARK2\n",
      "Word in BEL converted to unknown: GOBP:\"Wnt receptor signaling pathway\"\n",
      "Word in BEL converted to unknown: GOBP:\"Wnt receptor signaling pathway\"\n",
      "Word in BEL converted to unknown: HGNC:IL8\n",
      "Word in BEL converted to unknown: GOBP:\"patterning of blood vessels\"\n",
      "Word in BEL converted to unknown: GOBP:glycolysis\n",
      "Word in BEL converted to unknown: GOBP:\"Wnt receptor signaling pathway\"\n",
      "Word in BEL converted to unknown: GOBP:glycolysis\n",
      "Word in BEL converted to unknown: CHEBI:\"lactic acid\"\n",
      "Word in BEL converted to unknown: GOBP:\"patterning of blood vessels\"\n",
      "Word in BEL converted to unknown: GOBP:\"patterning of blood vessels\"\n",
      "Word in BEL converted to unknown: HGNC:IL8\n",
      "Word in BEL converted to unknown: HGNC:ERBB2IP\n",
      "Word in BEL converted to unknown: HGNC:ERBB2IP\n",
      "Word in BEL converted to unknown: HGNC:FIGF\n",
      "Word in BEL converted to unknown: HGNC:FIGF\n",
      "Word in BEL converted to unknown: HGNC:FIGF\n",
      "Word in BEL converted to unknown: HGNC:IL8\n",
      "Word in BEL converted to unknown: HGNC:IL8\n",
      "Word in BEL converted to unknown: HGNC:IL8\n",
      "Word in BEL converted to unknown: HGNC:IL8\n",
      "Word in BEL converted to unknown: HGNC:IL8\n",
      "Word in BEL converted to unknown: HGNC:IL8\n",
      "Word in BEL converted to unknown: HGNC:ATP5E\n",
      "Word in BEL converted to unknown: GOBP:\"patterning of blood vessels\"\n",
      "Word in BEL converted to unknown: GOBP:\"patterning of blood vessels\"\n",
      "Word in BEL converted to unknown: GOBP:\"Wnt receptor signaling pathway\"\n",
      "Word in BEL converted to unknown: GOBP:\"Wnt receptor signaling pathway\"\n",
      "Word in BEL converted to unknown: GOBP:\"Wnt receptor signaling pathway\"\n",
      "Word in BEL converted to unknown: GOBP:\"Wnt receptor signaling pathway\"\n",
      "Word in BEL converted to unknown: GOBP:\"Wnt receptor signaling pathway\"\n",
      "Word in BEL converted to unknown: GOBP:\"patterning of blood vessels\"\n",
      "Word in BEL converted to unknown: GOBP:\"patterning of blood vessels\"\n",
      "Word in BEL converted to unknown: GOBP:\"Wnt receptor signaling pathway\"\n",
      "Word in BEL converted to unknown: GOBP:\"Wnt receptor signaling pathway\"\n",
      "Word in BEL converted to unknown: GOBP:mitosis\n",
      "Word in BEL converted to unknown: GOBP:\"patterning of blood vessels\"\n",
      "Word in BEL converted to unknown: GOBP:\"Wnt receptor signaling pathway\"\n",
      "Word in BEL converted to unknown: GOBP:\"patterning of blood vessels\"\n",
      "Word in BEL converted to unknown: GOBP:\"patterning of blood vessels\"\n",
      "Word in BEL converted to unknown: GOBP:\"patterning of blood vessels\"\n",
      "Word in BEL converted to unknown: GOBP:glycolysis\n",
      "Word in BEL converted to unknown: GOBP:glycolysis\n",
      "Word in BEL converted to unknown: GOBP:mitosis\n",
      "Word in BEL converted to unknown: GOBP:\"Wnt receptor signaling pathway\"\n",
      "Word in BEL converted to unknown: GOBP:\"patterning of blood vessels\"\n",
      "Word in BEL converted to unknown: GOBP:\"Wnt receptor signaling pathway\"\n",
      "Word in BEL converted to unknown: GOBP:\"Wnt receptor signaling pathway\"\n",
      "Word in BEL converted to unknown: GOBP:\"Wnt receptor signaling pathway\"\n",
      "Word in BEL converted to unknown: HGNC:IL8\n",
      "Word in BEL converted to unknown: HGNC:IL8\n",
      "Word in BEL converted to unknown: HGNC:IL8\n",
      "Word in BEL converted to unknown: GOBP:glycolysis\n",
      "Word in BEL converted to unknown: HGNC:IL8\n",
      "Word in BEL converted to unknown: GOBP:\"patterning of blood vessels\"\n",
      "Word in BEL converted to unknown: HGNC:IL8\n",
      "Word in BEL converted to unknown: HGNC:MRE11A\n",
      "Word in BEL converted to unknown: HGNC:IL8\n",
      "Word in BEL converted to unknown: HGNC:IL8\n",
      "Word in BEL converted to unknown: HGNC:IL8\n",
      "Word in BEL converted to unknown: HGNC:IL8\n",
      "Word in BEL converted to unknown: GOBP:glycolysis\n",
      "Word in BEL converted to unknown: HGNC:ERO1L\n",
      "Word in BEL converted to unknown: GOBP:\"patterning of blood vessels\"\n",
      "Word in BEL converted to unknown: HGNC:MRE11A\n",
      "Word in BEL converted to unknown: GOBP:\"patterning of blood vessels\"\n",
      "Word in BEL converted to unknown: GOBP:\"patterning of blood vessels\"\n",
      "Word in BEL converted to unknown: HGNC:IL8\n",
      "Word in BEL converted to unknown: HGNC:IL8\n",
      "Word in BEL converted to unknown: HGNC:IL8\n",
      "Word in BEL converted to unknown: HGNC:IL8\n",
      "Word in BEL converted to unknown: HGNC:IL8\n",
      "Word in BEL converted to unknown: HGNC:IL8\n",
      "Word in BEL converted to unknown: HGNC:IL8\n",
      "Word in BEL converted to unknown: HGNC:IL8\n",
      "Word in BEL converted to unknown: HGNC:IL8\n",
      "Word in BEL converted to unknown: HGNC:IL8\n",
      "Word in BEL converted to unknown: HGNC:IL8\n",
      "Word in BEL converted to unknown: GOBP:\"patterning of blood vessels\"\n",
      "Word in BEL converted to unknown: HGNC:IL8\n",
      "Word in BEL converted to unknown: HGNC:IL8\n",
      "Word in BEL converted to unknown: HGNC:IL8\n",
      "Word in BEL converted to unknown: HGNC:IL8\n",
      "Deleted BEL sentence: act(p(MGI:Klra16)) -> p(HGNC:TYROBP,pmod(P))\n",
      "Deleted BEL sentence: a(CHEBI:\"N-acetyl-D-galactosamine\") -| p(MGI:Fus)\n",
      "Deleted BEL sentence: a(CHEBI:\"chondroitin sulfate\") -| p(MGI:Fus)\n",
      "Deleted BEL sentence: a(CHEBI:\"trichostatin A\") -| (act(p(MGI:Msx3)) -| p(HGNC:MSX1))\n",
      "Word in BEL converted to unknown: HGNC:ATP5A1\n",
      "Word in BEL converted to unknown: HGNC:ATP5A1\n",
      "Word in BEL converted to unknown: HGNC:ATP5B\n",
      "Word in BEL converted to unknown: HGNC:ATP5B\n",
      "Word in BEL converted to unknown: GOBP:mitosis\n"
     ]
    }
   ],
   "source": [
    "# Download text sentences\n",
    "TextSentenceID, vocabulary = loadAllTextSentences()\n",
    "\n",
    "# Download pre-trained word vectors\n",
    "word_vectors = KeyedVectors.load_word2vec_format('word_embeddings/PubMed-and-PMC-w2v.bin', binary=True)\n",
    "print('Finish loading word vectors from file') # e.g., word_vectors['increases']\n",
    "\n",
    "# Create text dict (requiring 1. vocabulary 2. word_vectors 3. T_vocab_size)\n",
    "T_idx2w, T_w2idx, T_vocab_size_total = createTextDict()\n",
    "\n",
    "# Create BEL Token dict (requiring 1. generalTokens 2. dicts of each namespace)\n",
    "B_idx2w, B_w2idx, B_vocab_size_total = createBELTokenDict()\n",
    "\n",
    "# Download (or creat, if not exists) word_embedding\n",
    "word_embedding = loadWordEmbeddings()\n",
    "\n",
    "# Download (or creat, if not exists) BELToken_embedding\n",
    "BELToken_embedding = loadBELTokenEmbeddings()\n",
    "\n",
    "# Load training and validation data\n",
    "trainT, trainB, trainZipsort = loadTrainingData('dataset/TrainingNormalised.BEL')\n",
    "sampleT, sampleB, sampleZipSort = loadTrainingData('dataset/SampleSetNormalised.BEL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [TL] EmbeddingInputlayer model/embedding/encode_seq_embedding: (50004, 200)\n",
      "  [TL] EmbeddingInputlayer model/embedding/decode_seq_embedding: (131832, 200)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [TL] DynamicRNNLayer model/seq2seq_encode: n_hidden:200, in_dim:3 in_shape:(16, ?, 200) cell_fn:BasicLSTMCell dropout:0.5 n_layer:1\n",
      "       batch_size (concurrent processes): 16\n",
      "  [TL] DynamicRNNLayerWithAttention model/seq2seq_decode: n_hidden:200, in_dim:3 in_shape:(16, ?, 200) cell_fn:BasicLSTMCell dropout:0.5 n_layer:1\n",
      "  [TL] DenseLayer  model/output: 131832 identity\n",
      "  [TL] EmbeddingInputlayer model/embedding/encode_seq_embedding: (50004, 200)\n",
      "  [TL] EmbeddingInputlayer model/embedding/decode_seq_embedding: (131832, 200)\n",
      "  [TL] DynamicRNNLayer model/seq2seq_encode: n_hidden:200, in_dim:3 in_shape:(1, ?, 200) cell_fn:BasicLSTMCell dropout:None n_layer:1\n",
      "       batch_size (concurrent processes): 1\n",
      "  [TL] DynamicRNNLayerWithAttention model/seq2seq_decode: n_hidden:200, in_dim:3 in_shape:(1, ?, 200) cell_fn:BasicLSTMCell dropout:None n_layer:1\n",
      "  [TL] DenseLayer  model/output: 131832 identity\n",
      "  [TL] EmbeddingInputlayer model/embedding/encode_seq_embedding: (50004, 200)\n",
      "  [TL] EmbeddingInputlayer model/embedding/decode_seq_embedding: (131832, 200)\n",
      "  [TL] DynamicRNNLayer model/seq2seq_encode: n_hidden:200, in_dim:3 in_shape:(1, ?, 200) cell_fn:BasicLSTMCell dropout:None n_layer:1\n",
      "       batch_size (concurrent processes): 1\n",
      "  [TL] DynamicRNNLayerWithAttention model/seq2seq_decode: n_hidden:200, in_dim:3 in_shape:(1, ?, 200) cell_fn:BasicLSTMCell dropout:None n_layer:1\n",
      "  [TL] DenseLayer  model/output: 131832 identity\n"
     ]
    }
   ],
   "source": [
    "# Define models (for train, test, validate)\n",
    "g = tf.Graph()\n",
    "with g.as_default() as graph:\n",
    "    tl.layers.clear_layers_name()\n",
    "\n",
    "    # model for training\n",
    "    encode_seqs = tf.placeholder(dtype=tf.int64, shape=[batch_size, None], name=\"encode_seqs\") # encoding input  ['It', 'was', 'choking', 'with', 'smoke', '.', '_', '_']\n",
    "    decode_seqs = tf.placeholder(dtype=tf.int64, shape=[batch_size, None], name=\"decode_seqs\") # decoding input  ['start_id', 'Nó', 'đặc', 'khói', '.', '_']\n",
    "    target_seqs = tf.placeholder(dtype=tf.int64, shape=[batch_size, None], name=\"target_seqs\") # decoding output ['Nó', 'đặc', 'khói', '.', 'end_id', '_']\n",
    "    target_mask = tf.placeholder(dtype=tf.int64, shape=[batch_size, None], name=\"target_mask\") # tl.prepro.sequences_get_mask()\n",
    "    net_out, _, net_encode, _, _, net_decode = modelWithAttention(encode_seqs, decode_seqs, is_train=True, reuse=False)\n",
    "\n",
    "    # model for inferencing\n",
    "    encode_seqs2 = tf.placeholder(dtype=tf.int64, shape=[1, None], name=\"encode_seqs\")\n",
    "    decode_seqs2 = tf.placeholder(dtype=tf.int64, shape=[1, None], name=\"decode_seqs\")\n",
    "    net, net_rnn, _, network_decode, cell, _ = modelWithAttention(encode_seqs2, decode_seqs2, is_train=False, reuse=True)\n",
    "    y = tf.nn.softmax(net.outputs)\n",
    "    \n",
    "    # model for validation\n",
    "    encode_seqs3 = tf.placeholder(dtype=tf.int64, shape=[1, None], name=\"encode_seqs\")\n",
    "    decode_seqs3 = tf.placeholder(dtype=tf.int64, shape=[1, None], name=\"decode_seqs\")\n",
    "    target_seqs3 = tf.placeholder(dtype=tf.int64, shape=[1, None], name=\"target_seqs\")\n",
    "    target_mask3 = tf.placeholder(dtype=tf.int64, shape=[1, None], name=\"target_mask\")\n",
    "    net_val, val_network_encode, _, val_network_decode, _, _  = modelWithAttention(encode_seqs3, decode_seqs3, is_train=False, reuse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [*] printing trainable variables\n",
      "  var   0: (50004, 200)      model/embedding/encode_seq_embedding/embeddings:0\n",
      "  var   1: (131832, 200)     model/embedding/decode_seq_embedding/embeddings:0\n",
      "  var   2: (400, 800)        model/seq2seq_encode/rnn/basic_lstm_cell/kernel:0\n",
      "  var   3: (800,)            model/seq2seq_encode/rnn/basic_lstm_cell/bias:0\n",
      "  var   4: (200, 200)        model/memory_layer/kernel:0\n",
      "  var   5: (600, 800)        model/seq2seq_decode/rnn/attention_wrapper/basic_lstm_cell/kernel:0\n",
      "  var   6: (800,)            model/seq2seq_decode/rnn/attention_wrapper/basic_lstm_cell/bias:0\n",
      "  var   7: (200, 131832)     model/output/W:0\n",
      "  var   8: (131832,)         model/output/b:0\n"
     ]
    }
   ],
   "source": [
    "# Define loss function and optimisation\n",
    "with g.as_default() as graph:\n",
    "    loss = tl.cost.cross_entropy_seq_with_mask(logits=net_out.outputs, target_seqs=target_seqs, input_mask=target_mask, return_details=False, name='cost')\n",
    "    lr = 0.001\n",
    "    train_op = tf.train.AdamOptimizer(learning_rate=lr).minimize(loss)\n",
    "    print_all_variables(train_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] Load T2BWithLuongAttentionDecode.npz SUCCESS!\n",
      "[*] Load T2BWithLuongAttentionEncode.npz SUCCESS!\n",
      "  [*] geting variables with model/memory_layer/kernel:0\n"
     ]
    }
   ],
   "source": [
    "# Initialise models\n",
    "with g.as_default() as graph:\n",
    "    sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True, log_device_placement=False))\n",
    "    tl.layers.initialize_global_variables(sess)\n",
    "    tl.files.assign_params(sess, [word_embedding], net_encode)\n",
    "    tl.files.assign_params(sess, [BELToken_embedding], net_decode)\n",
    "#     tl.files.load_and_assign_npz(sess=sess, name='T2BWithLuongAttention.npz', network=net)\n",
    "    tl.files.load_and_assign_npz(sess=sess, name='T2BWithLuongAttentionDecode.npz', network=net)\n",
    "    tl.files.load_and_assign_npz(sess=sess, name='T2BWithLuongAttentionEncode.npz', network=net_rnn)\n",
    "    b = get_variables_with_name(name='model/memory_layer/kernel:0')\n",
    "    c = tl.files.load_npz(path='', name='memory_layer.npz')\n",
    "    op_assign = b[0].assign(c[0])\n",
    "    sess.run(op_assign)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch[0/50] step:[0/657] loss:2.073379 took:7.12206s\n",
      "Epoch[0/50] step:[10/657] loss:1.729581 took:3.95501s\n",
      "Epoch[0/50] step:[20/657] loss:1.666680 took:7.21402s\n",
      "Epoch[0/50] step:[30/657] loss:1.562953 took:5.30498s\n",
      "Epoch[0/50] step:[40/657] loss:1.664813 took:5.23720s\n",
      "Input > MMP-2 gelatinolytic activity was higher in cells infected with PBS (mock) and Ad-SV.\n",
      "['MMP-2', 'gelatinolytic', 'activity', 'was', 'higher', 'in', 'cells', 'infected', 'with', 'PBS', '(', 'mock', ')', 'and', 'Ad-SV', '.']\n",
      "act(p(pmod(P))) -| act(p(())\n",
      "Input > In contrast, vandetanib treatment induced a 2.3-fold increase in eNOS mRNA in B16.F10 vasculature.\n",
      "['In', 'contrast', ',', 'vandetanib', 'treatment', 'induced', 'a', '2.3-fold', 'increase', 'in', 'eNOS', 'mRNA', 'in', 'B16.F10', 'vasculature', '.']\n",
      "act(p(pmod(P))) -| act(p(pmod(P)))\n",
      "Input > Thus, extramitochondrially targeted AIF is a dominant cell death inducer.\n",
      "['Thus', ',', 'extramitochondrially', 'targeted', 'AIF', 'is', 'a', 'dominant', 'cell', 'death', 'inducer', '.']\n",
      "act(p(pmod(P))) -> act(p(pmod(P)))\n",
      "Input > Binding of PIAS1 to human AR DNA+ligand binding domains was androgen dependent in the yeast liquid beta-galactosidase assay.\n",
      "['Binding', 'of', 'PIAS1', 'to', 'human', 'AR', 'DNA', '+', 'ligand', 'binding', 'domains', 'was', 'androgen', 'dependent', 'in', 'the', 'yeast', 'liquid', 'beta-galactosidase', 'assay', '.']\n",
      "p(HGNC:OSM) -| p(HGNC:OSM)\n",
      "Input > The data suggest that genistein may inhibit CFTR by two mechanisms.\n",
      "['The', 'data', 'suggest', 'that', 'genistein', 'may', 'inhibit', 'CFTR', 'by', 'two', 'mechanisms', '.']\n",
      "act(p(pmod(P))) -| act(p(pmod(P)))\n",
      "Input > LPS-induced NO synthesis feedback regulates itself through up-regulation of OPN promoter activity and gene transcription.\n",
      "['LPS-induced', 'NO', 'synthesis', 'feedback', 'regulates', 'itself', 'through', 'up-regulation', 'of', 'OPN', 'promoter', 'activity', 'and', 'gene', 'transcription', '.']\n",
      "p(HGNC:OSM) -> bp()\n",
      "Epoch[0/50] step:[50/657] loss:1.710430 took:6.56535s\n",
      "Epoch[0/50] step:[60/657] loss:1.649443 took:7.26760s\n",
      "Epoch[0/50] step:[70/657] loss:1.669503 took:7.95996s\n",
      "Epoch[0/50] step:[80/657] loss:1.580234 took:6.65523s\n",
      "Epoch[0/50] step:[90/657] loss:1.448144 took:5.90706s\n",
      "Input > MMP-2 gelatinolytic activity was higher in cells infected with PBS (mock) and Ad-SV.\n",
      "['MMP-2', 'gelatinolytic', 'activity', 'was', 'higher', 'in', 'cells', 'infected', 'with', 'PBS', '(', 'mock', ')', 'and', 'Ad-SV', '.']\n"
     ]
    }
   ],
   "source": [
    "# Train\n",
    "with g.as_default() as graph:\n",
    "    start_id = B_vocab_size_total-2\n",
    "    end_id = B_vocab_size_total-1\n",
    "    n_step = int(len(trainT)/batch_size)\n",
    "    for epoch in range(n_epoch):\n",
    "        epoch_time = time.time()\n",
    "        ## shuffle training data\n",
    "        trainT, trainB = shuffle(trainT, trainB, random_state=0)\n",
    "        ## train an epoch\n",
    "        total_err, n_iter = 0, 0\n",
    "        for X, Y in tl.iterate.minibatches(inputs=trainT, targets=trainB, batch_size=batch_size, shuffle=False):\n",
    "            step_time = time.time()\n",
    "\n",
    "            X = tl.prepro.pad_sequences(X)\n",
    "            X = [x[::-1] for x in X] # Reverse sequence\n",
    "            _target_seqs = tl.prepro.sequences_add_end_id(Y, end_id=end_id)\n",
    "            _target_seqs = tl.prepro.pad_sequences(_target_seqs)\n",
    "\n",
    "            _decode_seqs = tl.prepro.sequences_add_start_id(Y, start_id=start_id, remove_last=False)\n",
    "            _decode_seqs = tl.prepro.pad_sequences(_decode_seqs)\n",
    "            _target_mask = tl.prepro.sequences_get_mask(_target_seqs)\n",
    "\n",
    "            ## you can view the data here\n",
    "    #         for i in range(len(X)):\n",
    "    #             print(i, [E_idx2w[id] for id in X[i]])\n",
    "    #             print(i, [V_idx2w[id] for id in _target_seqs[i]])\n",
    "    #             print(i, [V_idx2w[id] for id in _decode_seqs[i]])\n",
    "    #             print(i, _target_mask[i])\n",
    "    #             print(len(_target_seqs[i]), len(_decode_seqs[i]), len(_target_mask[i]))\n",
    "            # exit()\n",
    "\n",
    "            _, err = sess.run([train_op, loss],\n",
    "                            {encode_seqs: X,\n",
    "                            decode_seqs: _decode_seqs,\n",
    "                            target_seqs: _target_seqs,\n",
    "                            target_mask: _target_mask})\n",
    "\n",
    "            if n_iter % 10 == 0:\n",
    "                print(\"Epoch[%d/%d] step:[%d/%d] loss:%f took:%.5fs\" % (epoch, n_epoch, n_iter, n_step, err, time.time() - step_time))\n",
    "\n",
    "            total_err += err; n_iter += 1\n",
    "\n",
    "            ###============= inference\n",
    "            if n_iter % 50 == 0:\n",
    "                testInference()\n",
    "\n",
    "        print(\"Epoch[%d/%d] averaged loss:%f took:%.5fs\" % (epoch, n_epoch, total_err/n_iter, time.time()-epoch_time))\n",
    "        \n",
    "        tl.files.save_npz(net.all_params, name='T2BWithLuongAttentionDecodePretrainedTokenEmb.npz', sess=sess)\n",
    "        tl.files.save_npz(net_rnn.all_params, name='T2BWithLuongAttentionEncodePretrainedTokenEmb.npz', sess=sess)\n",
    "        b = get_variables_with_name(name='model/memory_layer/kernel:0')\n",
    "        tl.files.save_npz(b, name='memory_layer_PretrainedTokenEmb.npz', sess=sess)\n",
    "        if epoch % 2 == 0:\n",
    "            print(\"Validation Results - Epoch: %d, Error: %f\" % (epoch, validate()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "testInference(visualiseAttention = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# sentences = loadSentences('dataset/TrainingNormalised.BEL')\n",
    "# trainBSentences = [line['BEL-normalised'] for line in sentences]\n",
    "# trainBTokenized = [tokeniseBEL(line) for line in trainBSentences]\n",
    "# trainTSentences = [TextSentenceID[line['Sentence-ID'][4:]]['text'] for line in sentences]\n",
    "# trainTTokenized = [TextSentenceID[line['Sentence-ID'][4:]]['tokens'] for line in sentences]\n",
    "# assert len(trainBTokenized) == len(trainTTokenized)\n",
    "# print(trainBTokenized[150], trainTTokenized[150])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use function loadTrainingData instead\n",
    "# sentences = loadSentences('dataset/SampleSetNormalised.BEL')\n",
    "# sampleBSentences = []\n",
    "# sampleBTokenized = []\n",
    "# sampleTSentences = []\n",
    "# sampleTTokenized = []\n",
    "# for line in sentences:\n",
    "#     BELTokens = tokeniseBEL(line['BEL-normalised'], mapToHGNC = True)\n",
    "#     if not BELTokens:\n",
    "#         print('Deleted BEL sentence:', line['BEL-normalised'])\n",
    "#         continue\n",
    "#     sampleBSentences.append(line['BEL-normalised'])\n",
    "#     sampleBTokenized.append(BELTokens)\n",
    "#     sampleTSentences.append(TextSentenceID[line['Sentence-ID'][4:]]['text'])\n",
    "#     sampleTTokenized.append(TextSentenceID[line['Sentence-ID'][4:]]['tokens'])\n",
    "# sampleT1 = [words2index(sublist, T_w2idx) for sublist in sampleTTokenized]\n",
    "# sampleB1 = [words2index(sublist, B_w2idx) for sublist in sampleBTokenized]\n",
    "# assert len(sampleT1) == len(sampleB1)\n",
    "# zipsort = sorted(zip(sampleT1,sampleB1), key=lambda pair: len(pair[0]))\n",
    "# sampleT = [x for x, y in zipsort]\n",
    "# sampleB = [y for x, y in zipsort] # Sort to make a batch have sentences with similar lengths\n",
    "# sampleZipSort = zipsort\n",
    "# print(sampleB[10], sampleT[10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# # model for training\n",
    "# encode_seqs = tf.placeholder(dtype=tf.int64, shape=[batch_size, None], name=\"encode_seqs\") # encoding input  ['It', 'was', 'choking', 'with', 'smoke', '.', '_', '_']\n",
    "# decode_seqs = tf.placeholder(dtype=tf.int64, shape=[batch_size, None], name=\"decode_seqs\") # decoding input  ['start_id', 'Nó', 'đặc', 'khói', '.', '_']\n",
    "# target_seqs = tf.placeholder(dtype=tf.int64, shape=[batch_size, None], name=\"target_seqs\") # decoding output ['Nó', 'đặc', 'khói', '.', 'end_id', '_']\n",
    "# target_mask = tf.placeholder(dtype=tf.int64, shape=[batch_size, None], name=\"target_mask\") # tl.prepro.sequences_get_mask()\n",
    "# net_out, _, net_encode = model(encode_seqs, decode_seqs, is_train=True, reuse=False)\n",
    "\n",
    "# # model for inferencing\n",
    "# encode_seqs2 = tf.placeholder(dtype=tf.int64, shape=[1, None], name=\"encode_seqs\")\n",
    "# decode_seqs2 = tf.placeholder(dtype=tf.int64, shape=[1, None], name=\"decode_seqs\")\n",
    "# net, net_rnn, _ = model(encode_seqs2, decode_seqs2, is_train=False, reuse=True)\n",
    "# y = tf.nn.softmax(net.outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Previous version of decoding for test data (Now, use the testModel function)\n",
    "# sentences = loadSentences('dataset/Task1NeuV3_corrected.sentence')\n",
    "# sampleTSentences = [TextSentenceID[line['Sentence-ID'][4:]]['text'] for line in sentences]\n",
    "# sampleTTokenized = [TextSentenceID[line['Sentence-ID'][4:]]['tokens'] for line in sentences]\n",
    "# sampleT = [words2index(sublist, T_w2idx) for sublist in sampleTTokenized]\n",
    "# f = open('results/Task1NeuV3_corrected' + '_' + strftime(\"%Y%m%d%H%M%S\", gmtime()) +'.txt', 'w')\n",
    "# for i in range(len(sampleT)):\n",
    "#     sentence_id = sentences[i]['Sentence-ID'][4:]\n",
    "#     seed_id = sampleT[i]\n",
    "#     # 1. encode, get state\n",
    "#     state = sess.run(net_rnn.final_state_encode,\n",
    "#                     {encode_seqs2: [seed_id]})\n",
    "#     # 2. decode, feed start_id, get first word\n",
    "#     #   ref https://github.com/zsdonghao/tensorlayer/blob/master/example/tutorial_ptb_lstm_state_is_tuple.py\n",
    "#     o, state = sess.run([y, net_rnn.final_state_decode],\n",
    "#                     {net_rnn.initial_state_decode: state,\n",
    "#                     decode_seqs2: [[start_id]]})\n",
    "#     w_id = tl.nlp.sample_top(o[0], top_k=1)\n",
    "#     w = B_idx2w[w_id]\n",
    "#     # 3. decode, feed state iteratively\n",
    "#     sentence = [w]\n",
    "#     for _ in range(30): # max sentence length\n",
    "#         o, state = sess.run([y, net_rnn.final_state_decode],\n",
    "#                         {net_rnn.initial_state_decode: state,\n",
    "#                         decode_seqs2: [[w_id]]})\n",
    "#         w_id = tl.nlp.sample_top(o[0], top_k=1)\n",
    "#         w = B_idx2w[w_id]\n",
    "#         if w_id == end_id:\n",
    "#             break\n",
    "#         sentence = sentence + [w]\n",
    "#     f.write(sentence_id + '\\t' + ''.join(sentence) + '\\n')\n",
    "# f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Old Inference code\n",
    "# seeds = [\"MMP-2 gelatinolytic activity was higher in cells infected with PBS (mock) and Ad-SV.\", \n",
    "#                          # p(HGNC:MMP2) increases act(p(HGNC:MMP2))\n",
    "#                         \"In contrast, vandetanib treatment induced a 2.3-fold increase in eNOS mRNA in B16.F10 vasculature.\", \n",
    "#                          # kin(p(MGI:Kdr)) decreases p(MGI:Nos3)\n",
    "#                         \"Thus, extramitochondrially targeted AIF is a dominant cell death inducer.\", \n",
    "#                          # p(MGI:Aifm1) increases bp(GOBP:\"cell death\")\n",
    "#                         \"Binding of PIAS1 to human AR DNA+ligand binding domains was androgen dependent in the yeast liquid beta-galactosidase assay.\", \n",
    "#                          # a(CHEBI:androgen) -> complex(p(HGNC:AR),p(HGNC:PIAS1))\"\n",
    "#                         \"The data suggest that genistein may inhibit CFTR by two mechanisms.\", \n",
    "#                          # a(CHEBI:genistein) decreases p(MGI:Cftr)\n",
    "#                         \"LPS-induced NO synthesis feedback regulates itself through up-regulation of OPN promoter activity and gene transcription.\" \n",
    "#                          # a(CHEBI:lipopolysaccharide) increases a(CHEBI:\"nitric oxide\"), p(MGI:Spp1) decreases a(CHEBI:\"nitric oxide\") \n",
    "#                         ] \n",
    "#                 for seed in seeds:\n",
    "#                     print(\"Input >\", seed)\n",
    "#                     output = nlp.annotate(seed, properties={'annotators': 'tokenize', 'outputFormat': 'json'})\n",
    "#                     seed_tokens = [i['originalText'] for i in output['tokens']]\n",
    "#                     seed_id = [T_w2idx[w] if w in T_w2idx else T_w2idx['unk'] for w in seed_tokens]\n",
    "#                     seed_id = seed_id[::-1]\n",
    "\n",
    "#     #                 # 1. encode, get state\n",
    "#     #                 state = sess.run(net_rnn.final_state_encode,\n",
    "#     #                                 {encode_seqs2: [seed_id]})\n",
    "#     #                 # 2. decode, feed start_id, get first word\n",
    "#     #                 #   ref https://github.com/zsdonghao/tensorlayer/blob/master/example/tutorial_ptb_lstm_state_is_tuple.py\n",
    "#     #                 o, state = sess.run([y, net_rnn.final_state_decode],\n",
    "#     #                                 {net_rnn.initial_state_decode: state,\n",
    "#     #                                 decode_seqs2: [[start_id]]})\n",
    "#     #                 w_id = tl.nlp.sample_top(o[0], top_k=1)\n",
    "#     #                 w = B_idx2w[w_id]\n",
    "#     #                 # 3. decode, feed state iteratively\n",
    "#     #                 sentence = [w]\n",
    "#     #                 for _ in range(30): # max sentence length\n",
    "#     #                     o, state = sess.run([y, net_rnn.final_state_decode],\n",
    "#     #                                     {net_rnn.initial_state_decode: state,\n",
    "#     #                                     decode_seqs2: [[w_id]]})\n",
    "#     #                     w_id = tl.nlp.sample_top(o[0], top_k=1)\n",
    "#     #                     w = B_idx2w[w_id]\n",
    "#     #                     if w_id == end_id:\n",
    "#     #                         break\n",
    "#     #                     sentence = sentence + [w]\n",
    "#     #                 print(\" >\", ' '.join(sentence))\n",
    "\n",
    "#                     # 1. encode, get state\n",
    "#                     state, memory = sess.run([net_rnn.final_state, net_rnn.outputs],\n",
    "#                                     {encode_seqs2: [seed_id]})\n",
    "# #                     print(state)\n",
    "#                     # 2. decode, feed start_id, get first word\n",
    "#                     #   ref https://github.com/zsdonghao/tensorlayer/blob/master/example/tutorial_ptb_lstm_state_is_tuple.py\n",
    "#                     feed_dict = {network_decode.initial_state.cell_state:state,\n",
    "#                                  network_decode.memory:memory,\n",
    "# #                                     encode_seqs2: [seed_id],\n",
    "#                                     decode_seqs2: [[start_id]]}\n",
    "# #                     print(feed_dict)\n",
    "#                     o, state = sess.run([y, network_decode.final_state],\n",
    "#                                     feed_dict)\n",
    "# #                     print(\"state\", state)\n",
    "# #                     print(\"outputs\", outputs)\n",
    "#                     w_id = tl.nlp.sample_top(o[0], top_k=1)\n",
    "#                     w = B_idx2w[w_id]\n",
    "#                     # 3. decode, feed state iteratively\n",
    "#                     sentence = [w]\n",
    "#                     for _ in range(30): # max sentence length\n",
    "#                         o, state = sess.run([y, network_decode.final_state],\n",
    "#                                         {network_decode.initial_state:state,\n",
    "#                                          network_decode.memory:memory,\n",
    "# #                                          encode_seqs2: [seed_id],\n",
    "#                                         decode_seqs2: [[w_id]]})\n",
    "#                         w_id = tl.nlp.sample_top(o[0], top_k=1)\n",
    "#                         w = B_idx2w[w_id] ###\n",
    "#                         if w_id == end_id:\n",
    "#                             break\n",
    "#                         sentence = sentence + [w]\n",
    "#                     print(\" >\", ' '.join(sentence))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
